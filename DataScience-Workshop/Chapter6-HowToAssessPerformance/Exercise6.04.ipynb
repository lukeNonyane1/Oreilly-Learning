{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Mean Absolute Error of a Second Model\n",
    "In this exercise, we will be engineering new features and finding the score and loss of a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the first step, you will import libraries such as train_test_split, LinearRegression, and mean_absolute_error. We make use of a pipeline to quickly transform our features and engineer new features using MinMaxScaler and PolynomialFeatures. MinMaxScaler reduces the variance in your data by adjusting all values to a range between 0 and 1. It does this by subtracting the mean of the data and dividing by the range, which is the minimum value subtracted from the maximum value. PolynomialFeatures will engineer new features by raising the values in a column up to a certain power and creating new columns in your DataFrame to accommodate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_headers = ['CIC0', 'SM1', 'GATS1i', 'NdsCH', 'Ndssc','MLOGP', 'response']\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/'\\\n",
    "                 'PacktWorkshops/The-Data-Science-Workshop'\\\n",
    "                 '/master/Chapter06/Dataset/qsar_fish_toxicity.csv', names=_headers, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIC0</th>\n",
       "      <th>SM1</th>\n",
       "      <th>GATS1i</th>\n",
       "      <th>NdsCH</th>\n",
       "      <th>Ndssc</th>\n",
       "      <th>MLOGP</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.260</td>\n",
       "      <td>0.829</td>\n",
       "      <td>1.676</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.453</td>\n",
       "      <td>3.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.189</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.348</td>\n",
       "      <td>3.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.125</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.348</td>\n",
       "      <td>3.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.027</td>\n",
       "      <td>0.331</td>\n",
       "      <td>1.472</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.807</td>\n",
       "      <td>3.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.094</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.886</td>\n",
       "      <td>5.390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIC0    SM1  GATS1i  NdsCH  Ndssc  MLOGP  response\n",
       "0  3.260  0.829   1.676      0      1  1.453     3.770\n",
       "1  2.189  0.580   0.863      0      0  1.348     3.115\n",
       "2  2.125  0.638   0.831      0      0  1.348     3.531\n",
       "3  3.027  0.331   1.472      1      0  1.807     3.510\n",
       "4  2.094  0.827   0.860      0      0  1.886     5.390"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "features = df.drop('response', axis=1).values\n",
    "labels = df[['response']].values\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(features, labels, test_size=0.2, random_state=0)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_eval, y_eval, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you begin by splitting the DataFrame called df into two. The first DataFrame is called features and contains all of the independent variables that you will use to make your predictions. The second is called labels and contains the values that you are trying to predict.\n",
    "\n",
    "In the third line, you split features and labels into four sets using train_test_split. X_train and y_train contain 80% of the data and are used for training your model. X_eval and y_eval contain the remaining 20%.\n",
    "\n",
    "In the fourth line, you split X_eval and y_eval into two additional sets. X_val and y_val contain 75% of the data because you did not specify a ratio or size. X_test and y_test contain the remaining 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline and engineer quadratic features\n",
    "steps = [('scaler', MinMaxScaler()), \n",
    "         ('poly', PolynomialFeatures(2)), \n",
    "         ('model', LinearRegression())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you begin by creating a Python list called steps. The list contains three tuples, each one representing a transformation of a model. The first tuple represents a scaling operation. The first item in the tuple is the name of the step, which you call scaler. This uses MinMaxScaler to transform the data. The second, called poly, creates additional features by crossing the columns of data up to the degree that you specify. In this case, you specify 2, so it crosses these columns up to a power of 2. Next comes your LinearRegression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple Linear Regression model with pipeline\n",
    "model = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you create an instance of Pipeline and store it in a variable called model. Pipeline performs a series of transformations, which are specified in the steps you defined in the previous step. This operation works because the transformers (MinMaxScaler and PolynomialFeatures) implement two methods called fit() and fit_transform(). You may recall from previous examples that models are trained using the fit() method that LinearRegression implements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', MinMaxScaler()), ('poly', PolynomialFeatures()),\n",
       "                ('model', LinearRegression())])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, X_train will be scaled. Next, additional features will be engineered. Finally, training will happen using the LinearRegression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model to predict on validation dataset\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.660552610083608\n"
     ]
    }
   ],
   "source": [
    "# compute MAE\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print('MAE: {}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss that you compute at this step is called a validation loss because you make use of the validation dataset. This is different from a training loss that is computed using the training dataset. This distinction is important to note as you study other documentation or books, which might refer to both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.6284921344153387\n"
     ]
    }
   ],
   "source": [
    "# compute R2 score\n",
    "r2 = model.score(X_val, y_val)\n",
    "print('R^2: {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should see a difference between the R2 score and the MAE of the first model and the second model (in the first model, the MAE and R2 scores were 0.781629 and 0.498688 respectively).\n",
    "\n",
    "In this exercise, you engineered new features that give you a model with a hypothesis of a higher polynomial degree. This model should perform better than simpler models up to a certain point. After engineering and training the new model, you computed the R2 score and MAE, which you can use to compare this model with the model you trained previously. We can conclude that this model is better as it has a higher R2 score and a lower MAE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
