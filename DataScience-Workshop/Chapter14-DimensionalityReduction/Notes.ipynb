{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction Strategies"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Backward Feature Elimination\n",
    "Forward Selection\n",
    "Principal Component Analysis (PCA)\n",
    "Independent Component Analysis (ICA)\n",
    "Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Feature Elimination (Recursive Feature Elimination)\n",
    "The mechanism behind the backward feature elimination algorithm is the recursive elimination of features and building a model on those features that remain after all the elimination.\n",
    "\n",
    "For example, let's take the case of the original dataset we had, which had 1,558 features. The algorithm starts off with all the 1,558 features in the first iteration.\n",
    "\n",
    "In the next step, we remove one feature at a time and train a model with the remaining n-1 features. This process is repeated n times. For example, we first remove feature 1 and then fit a model using all the remaining 1,557 variables. In the next iteration, we use feature 1 and instead, we eliminate feature 2 and then fit the model. This process is repeated n times (1,558) times.\n",
    "\n",
    "For each of the models fitted, the performance of the model (using measures such as accuracy) is calculated.\n",
    "The feature whose replacement has resulted in the smallest change in performance is removed permanently and Step 2 is repeated with n-1 features.\n",
    "The process is then repeated with n-2 features and so on.\n",
    "The algorithm keeps on eliminating features until the threshold number of features we require is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Feature Selection\n",
    "Forward feature selection works in the reverse order as backward elimination. In this process, we start off with an initial feature, and features are added one by one until no improvement in performance is achieved. The detailed process is as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "    -Start model building with one feature.\n",
    "    -Iterate the model building process n times, each time selecting one feature at a time. The feature that gives the highest improvement in performance is selected.\n",
    "    -Once the first feature is selected, it is the time to select the second feature. The process for selecting the second feature proceeds exactly the same as step 2. The remaining n-1 features are iterated along with the first feature and the performance on the model is observed. The feature that produces the biggest improvement in model performance is selected as the second feature.\n",
    "    -The iteration of features will continue until a threshold number of features we have determined is extracted.\n",
    "    -The set of final features selected will be the ones that give the maximum model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "Aims to find uncorrelated sources of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Component Analysis (ICA)\n",
    "ICA is a technique of dimensionality reduction that conceptually follows a similar path as PCA. Both ICA and PCA try to derive new sources of data by linearly combining the original data.\n",
    "\n",
    "While PCA attempts to find uncorrelated sources of data, ICA attempts to find independent sources of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "**Backward feature elimination** worked on the principle of eliminating features one by one until no major degradation of accuracy measures occurred. This method is computationally intensive, but we got better results than the benchmark model.\n",
    "\n",
    "**Forward feature selection** goes in the opposite direction as backward elimination and selects one feature at a time to get the best set of features we predetermined. This method is also computationally intensive. We also found out that this method had marginally lower accuracy.\n",
    "\n",
    "**Principal component analysis (PCA)** aims at finding components that are orthogonal to each other and that best explain the variability of the data. We had better results with PCA than we got from the benchmark model.\n",
    "\n",
    "**Independent component analysis (ICA)** is similar to PCA; however, it differs in terms of the approach to the selection of components. ICA looks for independent components from the dataset. We saw that ICA achieved one of the worst results in our context.\n",
    "\n",
    "**Factor analysis** was all about finding factors or groups of correlated features that best described the data. We achieved much better results than ICA with factor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
