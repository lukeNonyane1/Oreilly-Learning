{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This chapter will show you how to interpret a machine learning model's results and get deeper insights into the patterns it found. By the end of the chapter, you will be able to analyze weights from linear models and variable importance for RandomForest. You will be able to implement variable importance via permutation to analyze feature importance. You will use a partial dependence plot to analyze single variables and make use of the lime package for local interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance via Permutation\n",
    "Rather than generating random values, we can simply swap (or permute) values of a column between different rows and use these modified cases for predictions. Then, we can calculate the related accuracy score and compare it with the original one to assess the importance of this variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feature_importance_permutation** takes the following parameters:\n",
    "\n",
    "    - predict_method: A function that will be called for model prediction. \n",
    "    - X: The features from the dataset. It needs to be in NumPy array form.\n",
    "    - y: The target variable from the dataset. It needs to be in Numpy array form.\n",
    "    metric: The metric used for comparing the performance of the model. For the classification task, we will use accuracy.\n",
    "    - num_round: The number of rounds mlxtend will perform permutation on the data and assess the performance change.\n",
    "    - seed: The seed set for getting reproducible results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependence Plots\n",
    "Another tool that is model-agnostic is a partial dependence plot. It is a visual tool for analyzing the effect of a feature on the target variable. To achieve this, we can plot the values of the feature we are interested in analyzing on the x-axis and the target variable on the y-axis and then show all the observations from the dataset on this graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Interpretation with LIME \n",
    "After training our model, we usually use it for predicting outcomes on unseen data. The global interpretations we saw earlier, such as model coefficient, variable importance, and the partial dependence plot, gave us a lot of information on the features at an overall level. Sometimes we want to understand what has influenced the model for a specific case to predict a specific outcome. For instance, if your model is to assess the risk of offering credit to a new client, you may want to understand why it rejected the case for a specific lead. This is what local interpretation is for: analyzing a single observation and understanding the rationale behind the model's decision. In this section, we will introduce you to a technique called **Locally Interpretable Model-Agnostic Explanations (LIME)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this chapter, we learned a few techniques for interpreting machine learning models. We saw that there are techniques that are specific to the model used: coefficients for linear models and variable importance for tree-based models. There are also some methods that are model-agnostic, such as variable importance via permutation.\n",
    "\n",
    "All these techniques are global interpreters, which look at the entire dataset and analyze the overall contribution of each variable to predictions. We can use this information not only to identify which variables have the most impact on predictions but also to shortlist them. Rather than keeping all features available from a dataset, we can just keep the ones that have a stronger influence. This can significantly reduce the computation time for training a model or calculating predictions.\n",
    "\n",
    "We also went through a local interpreter scenario with LIME, which analyzes a single observation. It helped us to better understand the decisions made by the model in predicting the final outcome for a given case. This is a very powerful tool to assess whether a model is biased toward a specific variable that could contain sensitive information such as personal details or demographic data. We can also use it to compare two different observations and understand the rationale for getting different outcomes from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
