{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ffbd56ad-9780-482e-b928-cb02ad5c2783",
   "metadata": {},
   "source": [
    "In this activity, we will train a CNN to recognize images of fruits that belong to 120 different classes. We will use transfer learning and data augmentation to do so. We will be using the Fruits 360 dataset (https://arxiv.org/abs/1712.00580).\n",
    "It contains more than 82,000 images of 120 different types of fruits. We will be using a subset of this dataset with more than 16,000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcedfc5c-7344-4953-8262-4e2533ab446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b14cb268-39ad-4e1c-b7d2-e992a0aec23a",
   "metadata": {},
   "source": [
    "Create a variable called file_url containing the link to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536cf420-9fd5-427c-bd2d-265ce76c936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://github.com/lukeNonyane1/The-Deep-Learning-Workshop/raw/master/Chapter03/Datasets/Activity3.02/fruits360.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd59c021-b29a-4968-b744-982ea8ce96cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "zip_dir = tf.keras.utils.get_file('fruits360', origin=file_url, extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3309ea-122f-42d7-9aa3-7896fe2dd369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/LNonyane/.keras/datasets/fruits360'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa0c9c1-ce05-4dd2-8289-49ac4c7cc8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pathlib library\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ff1671-6031-4543-bfb1-4201740b57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable called path containing the full path to the fruits360_filtered directory using pathlib.Path(zip_dir).parent\n",
    "path = pathlib.Path(zip_dir).parent / 'fruits360_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e93b8d50-4c0c-4941-8c24-bed691238f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/LNonyane/.keras/datasets\n"
     ]
    }
   ],
   "source": [
    "print(path.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f4b025e-6650-4ed6-9859-c851a10f44cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/LNonyane/.keras/datasets/fruits360_filtered/Test'),\n",
       " PosixPath('/Users/LNonyane/.keras/datasets/fruits360_filtered/Training')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterate and print each directory \n",
    "[x for x in path.iterdir() if x.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e5de9b-a2f6-4e5b-8fb4-c399015f3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two variables called train_dir and val_dir that take the full paths to the train and validation folders, respectively\n",
    "train_dir = path / 'Training'\n",
    "val_dir = path / 'Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99ccaec0-b923-468e-8dc4-1239bcb32a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_train and total_val\n",
    "total_train, total_val = 11398, 4752 # values provided"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44a34297-409c-4213-9c35-261017225937",
   "metadata": {},
   "source": [
    "Create a data generator with the following data augmentation:\n",
    "rescale=1./255,\n",
    "\n",
    "rotation_range=40,\n",
    "\n",
    "width_shift_range=0.1,\n",
    "\n",
    "height_shift_range=0.1,\n",
    "\n",
    "shear_range=0.2,\n",
    "\n",
    "zoom_range=0.2,\n",
    "\n",
    "horizontal_flip=True,\n",
    "\n",
    "fill_mode='nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9c98e3-f098-463c-acbe-129410a1adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "836f821a-6a17-4c17-9c29-f5b1e4b4f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_gen\n",
    "train_img_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841e10f-f793-4bfe-9030-96aa14bbed22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc513b57-2d88-4c42-98ea-bcde93d72f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cad86a9-7daa-424d-ae9e-60adf8675fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, img_heght, img_width, channel\n",
    "batch_size, img_heght, img_width, channel = 16, 100, 100, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "352f2f6c-6bbe-49b1-89b9-2d4d1de07533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11398 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "# create a data generator called train_data_gen using flow_from_directory\n",
    "train_data_gen = train_img_gen.flow_from_directory(batch_size=batch_size,\n",
    "                                                  directory=train_dir,\n",
    "                                                  target_size=(img_heght, img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6412a7f6-1d74-4266-a738-a89572a08011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c1c3d63-032c-4863-89eb-ff684ab4c323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4752 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "# create a data generator called val_data_gen using flow_from_directory\n",
    "val_data_gen = val_img_gen.flow_from_directory(batch_size=batch_size,\n",
    "                                                  directory=val_dir,\n",
    "                                                  target_size=(img_heght, img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d823af62-badb-4aae-99f9-54eade972849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a6b0195-e8da-4245-9850-d7f4f1aa7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 8 as the seed for numpy and tensorflow using np.random.seed(8) and tf.random.set_seed(8)\n",
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5ca77df-db27-4420-be62-171baf09ec7b",
   "metadata": {},
   "source": [
    "Load a pretrained VGG16 model from TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a92fc75b-f86d-435d-bf42-6c51ffbd8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VGG16\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22995928-bd62-4523-b953-85462021442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 14:37:36.659721: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# base_model\n",
    "base_model = VGG16(input_shape=(img_heght,\n",
    "                                img_width,\n",
    "                                channel),\n",
    "                   weights='imagenet',\n",
    "                   include_top=False)\n",
    "\n",
    "# freeze model so that weights will not be updated\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e31e342f-6158-4e96-9a62-b88fddd5cdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 100, 100, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 100, 100, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 50, 50, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 50, 50, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 25, 25, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 25, 25, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 12, 12, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# base_model summary\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71196eea-69cd-4a8b-8233-f923cfcdad62",
   "metadata": {},
   "source": [
    "This output shows us the architecture of VGG16. We can see that there are 14,714,688 parameters in total, but there is no trainable parameter. This is expected as we have frozen all the layers of this model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "98f24d32-dcdb-4409-8ebf-46a394d1dbc3",
   "metadata": {},
   "source": [
    "Add two fully connected layers on top of VGG16: A fully connected layer with Dense(1000, activation='relu') and a fully connected layer with Dense(120, activation='softmax')."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f530b8a3-fdc9-4019-ba90-60e0399858b2",
   "metadata": {},
   "source": [
    "# prediction_layers\n",
    "prediction_layers = tf.keras.Sequential([layers.Dense(1000, activation='relu'),\n",
    "                                       layers.Dense(120, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "377b2029-5713-4e6f-9f83-90b95ec51c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fully connected layers to base_model\n",
    "new_model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1000, activation='relu'),\n",
    "    layers.Dense(120, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43c04042-7af1-4249-b4d9-dfa50d8e2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the prediction_layers' weights will be updated when the model is trained\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "new_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "067eed4d-5556-425d-9b43-3130f506ab3c",
   "metadata": {},
   "source": [
    "To use a pretrained model, we need to import its implemented class. Here, we will be importing a VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0574869-2aa4-43f8-8ab5-bdb64bd2d810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 3, 3, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              4609000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 120)               120120    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,443,808\n",
      "Trainable params: 4,729,120\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summary of new model\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ab13906-4869-4037-8841-7659871a8a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fm/b49y7nm968107m07lbt3cqsc0000gp/T/ipykernel_47437/4064329950.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  new_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 292s 409ms/step - loss: 2.0464 - accuracy: 0.5031 - val_loss: 1.0113 - val_accuracy: 0.7302\n",
      "Epoch 2/5\n",
      "712/712 [==============================] - 320s 450ms/step - loss: 0.6719 - accuracy: 0.8004 - val_loss: 0.6865 - val_accuracy: 0.8131\n",
      "Epoch 3/5\n",
      "712/712 [==============================] - 323s 454ms/step - loss: 0.4499 - accuracy: 0.8629 - val_loss: 0.6164 - val_accuracy: 0.8207\n",
      "Epoch 4/5\n",
      "712/712 [==============================] - 324s 455ms/step - loss: 0.3380 - accuracy: 0.8944 - val_loss: 0.4187 - val_accuracy: 0.8721\n",
      "Epoch 5/5\n",
      "712/712 [==============================] - 319s 449ms/step - loss: 0.2800 - accuracy: 0.9113 - val_loss: 0.3994 - val_accuracy: 0.8849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7facfaafe6d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model and provide the train and validation data generators, epochs=5, and the validation steps\n",
    "new_model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=5,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf880e-b4f5-4260-8661-67b4754a78fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e0f129-c8a6-4db4-95f2-e68c42bed290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
