{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb1b6b1-d04a-410e-a987-90513d539c94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Project Description and Scope: \n",
    "\n",
    "You are provided with a collection of images of pets, that is, cats and dogs. These images are of different sizes with varied lighting conditions and they should be used as inputs for your model.\n",
    "You are expected to write the code for CNN image classification model using TensorFlow that trains on the data and calculates the accuracy score on the test data. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "38863501-76e3-4dbe-81a2-4d31227fbb48",
   "metadata": {},
   "source": [
    "Begin by creating the ipynb file in the same parent folder where the downloaded data set is kept. The CNN model should have the following layers: \n",
    "● Input layer \n",
    "● Convolutional layer 1 with 32 filters of kernel size[5,5] \n",
    "● Pooling layer 1 with pool size[2,2] and stride 2 \n",
    "● Convolutional layer 2 with 64 filters of kernel size[5,5] \n",
    "● Pooling layer 2 with pool size[2,2] and stride 2 \n",
    "● Dense layer whose output size is fixed in the hyper parameter: fc_size=32 \n",
    "● Dropout layer with dropout probability 0.4 \n",
    "Predict the class by doing a softmax on the output of the dropout layers. \n",
    "This should be followed by training and evaluation: \n",
    "● For the training step, define the loss function and minimize it \n",
    "● For the evaluation step, calculate the accuracy \n",
    "Run the program for 100, 200, and 300 iterations, respectively. Follow this by a report on the final accuracy and loss on the evaluation data. \n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "To execute this project, please ensure you have the latest version of TensorFlow installed on your system.\n",
    "To download the datasets: https://lms.simplilearn.com/user/project/download-attachment?file=1577957291_deeplearningwithkerasandtensorflow.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ca0593-782b-4f21-b9b5-d6c033318d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d5a991d-e420-4e19-a907-0afcc432b309",
   "metadata": {},
   "source": [
    "Create a variable called file_url containing the link to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5e2651-48e4-4a39-bc6b-4fbf3baf1bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://github.com/lukeNonyane1/Oreilly-Learning/raw/main/deep-learning-with-keras-tensorflow-workshop/ch3-image-classification-with-CNNs/1577957291_deeplearningwithkerasandtensorflow.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ec4375-13af-4e05-904c-3b98ead4a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "zip_dir = tf.keras.utils.get_file('pet_classification', origin=file_url, extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55c2336-3f15-49be-bd0f-f6fbf068d31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/LNonyane/.keras/datasets/pet_classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16179eea-82ce-4ca4-ad66-c8974a19e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pathlib library\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "raw",
   "id": "020eb006-0bd1-41cb-b1fe-2e08281b8776",
   "metadata": {},
   "source": [
    "This module offers classes representing filesystem paths with semantics appropriate for different operating systems. Path classes are divided between pure paths, which provide purely computational operations without I/O, and concrete paths, which inherit from pure paths but also provide I/O operations.\n",
    "It instantiates a concrete path for the platform the code is running on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff629cc6-2fa6-4a5c-b7e1-80742ca51ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable called path containing the full path to the data directory using pathlib.Path(zip_dir).parent\n",
    "path = pathlib.Path(zip_dir).parent / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48276c85-75a0-46d3-8197-f320c0734214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/LNonyane/.keras/datasets\n"
     ]
    }
   ],
   "source": [
    "print(path.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbd4a1cb-6006-47f5-b2ad-bae14204fdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/LNonyane/.keras/datasets/data/test'),\n",
       " PosixPath('/Users/LNonyane/.keras/datasets/data/train')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterate and print each directory \n",
    "[x for x in path.iterdir() if x.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f09608-9bbc-42bf-8e61-658890131396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two variables called train_dir and val_dir that take the full paths to the train and validation folders, respectively\n",
    "train_dir = path / 'train'\n",
    "val_dir = path / 'test'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f85f3e9-fc1b-4845-9a1d-9aad76667895",
   "metadata": {},
   "source": [
    "Create four variables called train_cats_dir, train_dogs_dir, val_cats_dir, and val_dogs_dir that take the full paths to the cats and dogs folders for the train and validation sets, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380e937a-5617-4d09-8c75-ceb41658bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats_dir = train_dir / 'cats'\n",
    "train_dogs_dir = train_dir / 'dogs'\n",
    "val_cats_dir = val_dir / 'cats'\n",
    "val_dogs_dir = val_dir / 'dogs'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4625c9b6-a417-48f9-b2ad-819d18312d9d",
   "metadata": {},
   "source": [
    "Import the os package. We will need this in the next step in order to count the number of images from a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f63f8ee-ab8c-4025-8dfa-89da3761d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66d39c97-c8cc-4354-b029-1854e7aae0f8",
   "metadata": {},
   "source": [
    "Create two variables called total_train and total_val that will get the number of images for the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2acb4b00-e52e-4082-8a35-59c3c2d9f01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train: 40\n",
      "total validation: 20\n"
     ]
    }
   ],
   "source": [
    "total_train = len(os.listdir(train_cats_dir)) + len(os.listdir(train_dogs_dir))\n",
    "total_val = len(os.listdir(val_cats_dir)) + len(os.listdir(val_dogs_dir))\n",
    "print('total train:',total_train)\n",
    "print('total validation:',total_val)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "faee3976-590d-4f0a-aa2e-72b8af1d4b84",
   "metadata": {},
   "source": [
    "Create a data generator with the following data augmentation:\n",
    "rescale=1./255,\n",
    "\n",
    "rotation_range=40,\n",
    "\n",
    "width_shift_range=0.1,\n",
    "\n",
    "height_shift_range=0.1,\n",
    "\n",
    "shear_range=0.2,\n",
    "\n",
    "zoom_range=0.2,\n",
    "\n",
    "horizontal_flip=True,\n",
    "\n",
    "fill_mode='nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a39034-37e0-4976-a811-30f37dec4ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af63c30-fdbf-42ee-a699-82622849633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_gen rescaled\n",
    "train_img_gen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adb5fec7-deab-49c1-8dbf-fa20348f0371",
   "metadata": {},
   "source": [
    "We rescaling in order normalize all the images by dividing them by 255 so that all the pixels will have a value between 0 and 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ced6f69c-6b60-4a7b-99f2-898098dc19d2",
   "metadata": {},
   "source": [
    "# train_img_gen\n",
    "train_img_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb3008a4-3aae-4591-9c12-6e25265f47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b3c13eb-be02-4cd6-968f-8d9301e48bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, img_heght, img_width, channel\n",
    "batch_size, img_heght, img_width, channel = 4, 100, 100, 3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c3c0f77-830a-43f1-88bf-afc6cef2f9a3",
   "metadata": {},
   "source": [
    "batch_size allows data generators to divide images into batches and feed them sequentially to the model instead of loading the entire data set into memory.\n",
    "\n",
    "img_heght & img_width have been set to arbitrary values.\n",
    "\n",
    "channel represents whether image is grayscale or color: 1=grayscale, 3=color\n",
    "\n",
    "CNN require 3D images hence the channel is being added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf848c61-8e54-4400-9e90-0b444f70d63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create a data generator called train_data_gen using flow_from_directory\n",
    "train_data_gen = train_img_gen.flow_from_directory(batch_size=batch_size,\n",
    "                                                  directory=train_dir,\n",
    "                                                  target_size=(img_heght, img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e8f866e-7633-4e9c-b85b-7ca0a019f897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create a data generator called val_data_gen using flow_from_directory\n",
    "val_data_gen = val_img_gen.flow_from_directory(batch_size=batch_size,\n",
    "                                                  directory=val_dir,\n",
    "                                                  target_size=(img_heght, img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6149f3d6-74ad-449d-b087-170e4eeaffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "088ea53c-bc24-4d3e-9d2c-75e469e4818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 8 as the seed for numpy and tensorflow using np.random.seed(8) and tf.random.set_seed(8); these values are arbitrary.\n",
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49e88eb0-d0be-47f7-b21e-0d88311b765d",
   "metadata": {},
   "source": [
    "Instantiate a tf.keras.Sequential() class into a variable called model with the following layers: \n",
    "A convolution layer with 32 kernels of shape 5, ReLU as the activation function, and the required input dimensions; \n",
    "A max pooling layer with size=2 and stride=2; \n",
    "A convolution layer with 64 kernels of shape 5 and ReLU as the activation function; \n",
    "A max pooling layer with size=2 and stride=2; \n",
    "A flatten layer; \"Flattens the input but does affect the batch size. Required before the first fully connected layer in order to flatten the featuremaps.\"\n",
    "A Dropout layer with dropout probability 0.4 [Dropout is regularization technique to avoid overfitting (increase the validation accuracy) thus increasing the generalizing power.]\n",
    "A softmax layer to predict the class on the output of the dropout layers. \n",
    "A fully connected layer with 32 units and ReLU as the activation function; \n",
    "A fully connected layer with 1 unit and sigmoid as the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02a67f16-cbb2-47ac-93d7-2e3c1be14023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 07:03:42.702007: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, 5, activation='relu', input_shape=(img_heght, img_width, 3)),\n",
    "    layers.MaxPool2D(pool_size=(2,2), strides=1),\n",
    "    layers.Conv2D(64, 5, activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=(2,2), strides=1),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(rate=0.4),\n",
    "    layers.Softmax(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3feac87a-8b09-4174-bb74-12a48ce7d4b5",
   "metadata": {},
   "source": [
    "Instantiate a tf.keras.optimizers.Adam() class with 0.001 as the learning rate and save it to a variable called optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbcc6c28-9b29-4d39-ad2e-09904147d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= tf.keras.optimizers.Adam(0.001)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce3a5165-9721-4e40-92fb-33fcc09828da",
   "metadata": {},
   "source": [
    "Compile the neural network using .compile() with loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc97fc3d-f3c6-4020-b495-53346a2b2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54258708-3ed3-4dcd-b730-8697bef04ae2",
   "metadata": {},
   "source": [
    "Print a summary of the model using .summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ba5193e-f190-4ce8-9c6e-15b260c0fac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 96, 96, 32)        2432      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 95, 95, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 91, 91, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 90, 90, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 518400)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                16588832  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,642,561\n",
      "Trainable params: 16,642,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0d396-5db0-4c57-93eb-9156be39c43b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fit the neural networks with fit() and provide the train and validation data generators, epochs=100, the steps per epoch, and the validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0c45f18-d439-40c2-9c88-8ff303a76cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 2s 180ms/step - loss: 0.6976 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 2s 167ms/step - loss: 0.6944 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 2s 163ms/step - loss: 0.6950 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 2s 164ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 2s 161ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83f74d9be0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=5,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac48dd90-d9ff-4f88-be87-82b1ee21b565",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fit the neural networks with fit() and provide the train and validation data generators, epochs=200, the steps per epoch, and the validation steps"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8214447f-5b5a-412a-98ea-6bfb45ee15be",
   "metadata": {},
   "source": [
    "model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=200,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d49d81e-1ef9-4639-b7c9-a683db116548",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fit the neural networks with fit() and provide the train and validation data generators, epochs=300, the steps per epoch, and the validation steps"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4eb94fd4-2eb1-4e3c-981f-58afbe90cb27",
   "metadata": {},
   "source": [
    "model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=300,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0064a-feaf-4803-b3ea-d74e42bd84f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Improving model by leveraging pre-trained CNN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5ec291e-ca70-4d75-a699-1841c5bbea5d",
   "metadata": {},
   "source": [
    "Load a pretrained VGG16 model from TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da2bf6ad-9e75-49ca-ae47-351759471fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VGG16\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45cdb981-e6c8-443f-a8be-8322254f7a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model\n",
    "base_model = VGG16(input_shape=(img_heght,\n",
    "                                img_width,\n",
    "                                channel),\n",
    "                   weights='imagenet',\n",
    "                   include_top=False)\n",
    "\n",
    "# freeze model so that weights will not be updated\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57c3d610-c9f1-4476-bac3-1e010ceb87a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 100, 100, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 100, 100, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 50, 50, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 50, 50, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 25, 25, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 25, 25, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 12, 12, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# base_model summary\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b18b20c-5854-41a0-ab7d-3c4943247129",
   "metadata": {},
   "source": [
    "This output shows us the architecture of VGG16. We can see that there are 14,714,688 parameters in total, but there is no trainable parameter. This is expected as we have frozen all the layers of this model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e25ae05-5806-4490-91f2-b13ba7d7950e",
   "metadata": {},
   "source": [
    "Add the following layers on top of VGG16: \n",
    "Fully connected layer with Dense(120, activation='relu')\n",
    "Fully connected layer with Dense(2, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6738243-b24c-4a71-8519-e8fe4b131fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fully connected layers to base_model\n",
    "new_model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93117c43-a462-4111-8863-be79c2a07b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the prediction_layers' weights will be updated when the model is trained\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "new_model.compile(loss='binary_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3d850eb-926b-4706-9a3d-c854ec63b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 3, 3, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 120)               553080    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 242       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,268,010\n",
      "Trainable params: 553,322\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summary of new model\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52ac1292-7a10-44da-892c-885c17dd7965",
   "metadata": {},
   "source": [
    "total_train, total_val = 40, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ef99f12-a975-4910-80f5-c7a574271e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 720ms/step - loss: 0.7027 - accuracy: 0.5000 - val_loss: 0.7607 - val_accuracy: 0.7500\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 1.6411 - accuracy: 0.2500 - val_loss: 0.9991 - val_accuracy: 0.2500\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 1.1886 - accuracy: 0.0000e+00 - val_loss: 0.6095 - val_accuracy: 0.7500\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.5819 - accuracy: 0.7500 - val_loss: 1.1460 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.5300 - accuracy: 0.7500 - val_loss: 1.2843 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83d71a4d90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model and provide the train and validation data generators, epochs=5, and the validation steps\n",
    "new_model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=1,\n",
    "    epochs=5,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5be2726-24c0-49b0-b8af-85c01c2ab09d",
   "metadata": {},
   "source": [
    "As we can see from the output above we were able to improve our validation accuracy to 75% from 50%. There's still room for optimization on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8d746-32c4-4cb4-967d-6f0bd3a4dc92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
