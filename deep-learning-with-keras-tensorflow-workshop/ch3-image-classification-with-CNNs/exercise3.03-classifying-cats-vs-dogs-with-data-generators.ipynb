{
 "cells": [
  {
   "cell_type": "raw",
   "id": "da4168b3-917a-4a65-b0a0-7c8d6186115e",
   "metadata": {},
   "source": [
    "In this exercise, we will be working on the cats versus dogs dataset, which contains images of dogs and cats. We will build two data generators for the training and validation sets and a CNN model to recognize images of dogs or cats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1543eb-ce29-4162-98e0-01dcc440c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "beae63f0-f495-4cbe-ad47-0a62002ce2db",
   "metadata": {},
   "source": [
    "Create a variable called file_url containing the link to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f66002d-2af7-4438-a559-446f525f4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://github.com/PacktWorkshops/The-Deep-Learning-Workshop/raw/master/Chapter03/Datasets/Exercise3.03/cats_and_dogs_filtered.zip'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "077052c3-ea5e-42a0-96aa-0cbad984d5a3",
   "metadata": {},
   "source": [
    "Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23e7607-1fd6-4cc1-ba9a-9b682d452186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/PacktWorkshops/The-Deep-Learning-Workshop/raw/master/Chapter03/Datasets/Exercise3.03/cats_and_dogs_filtered.zip\n",
      "68608000/68606236 [==============================] - 5s 0us/step\n",
      "68616192/68606236 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "zip_dir = tf.keras.utils.get_file('cats_and_dogs.zip', origin=file_url, extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c66ead-8a8e-4aea-8737-6cb763246863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/LNonyane/.keras/datasets/cats_and_dogs.zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_dir"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9d1a71a-3247-4420-ad49-b5f175fbe77e",
   "metadata": {},
   "source": [
    "Import the pathlib library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df51027-ef4e-409c-a54a-e599a8f896f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec1a132a-e77a-4b25-8046-0d31758032d5",
   "metadata": {},
   "source": [
    "Create a variable called path containing the full path to the cats_and_dogs_filtered directory using pathlib.Path(zip_dir).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e01df3a4-d117-4fc6-8bf5-8a375f1d7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path(zip_dir).parent / 'cats_and_dogs_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b7ef22-e5ca-4284-801e-115424fcafd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/LNonyane/.keras/datasets\n"
     ]
    }
   ],
   "source": [
    "print(path.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e7424d7-5b59-4cd7-ac90-fa7ad3580df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/LNonyane/.keras/datasets/cats_and_dogs_filtered/train'),\n",
       " PosixPath('/Users/LNonyane/.keras/datasets/cats_and_dogs_filtered/validation')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in path.iterdir() if x.is_dir()]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f655ab04-d778-47f4-883e-14e7498f4c66",
   "metadata": {},
   "source": [
    "Create two variables called train_dir and validation_dir that take the full paths to the train and validation folders, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2702e60b-12b5-476d-b42c-1f80484adc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = path / 'train'\n",
    "validation_dir = path / 'validation'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7c16867-1338-4043-98d3-1e15667977a3",
   "metadata": {},
   "source": [
    "Create four variables called train_cats_dir, train_dogs_dir, validation_cats_dir, and validation_dogs_dir that take the full paths to the cats and dogs folders for the train and validation sets, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e0b022b-dde1-4eae-a8f6-21dd847a30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats_dir = train_dir / 'cats'\n",
    "train_dogs_dir = train_dir / 'dogs'\n",
    "validation_cats_dir = validation_dir / 'cats'\n",
    "validation_dogs_dir = validation_dir / 'dogs'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffe25db2-9478-45f5-b175-6b6410280fb5",
   "metadata": {},
   "source": [
    "Import the os package. We will need this in the next step in order to count the number of images from a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4861624b-e28d-482d-a911-63ca60db6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17042f70-dbc4-4f79-a67e-b06cadf5f6d9",
   "metadata": {},
   "source": [
    "Create two variables called total_train and total_val that will get the number of images for the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e81967e-a41c-4739-91e7-f5f5dd17412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train = len(os.listdir(train_cats_dir)) + len(os.listdir(train_dogs_dir))\n",
    "total_val = len(os.listdir(validation_cats_dir)) + len(os.listdir(validation_dogs_dir))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af88e3d1-368f-47b8-b9ff-57a1556f8ba9",
   "metadata": {},
   "source": [
    "Import ImageDataGenerator from tensorflow.keras.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37e654dd-66cf-45ac-b080-5cdd31c69c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6caf663-042e-4371-b731-9b642e148bbf",
   "metadata": {},
   "source": [
    "Instantiate two ImageDataGenerator classes and call them train_image_generator and validation_image_generator. These will rescale the images by dividing them by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa157add-3ed1-4023-88d7-982a00d3a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(rescale=1./255)\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8947737-1371-41af-bcb5-f5f7bc3ef541",
   "metadata": {},
   "source": [
    "Create three variables called batch_size, img_height, and img_width that take the values 16, 100, and 100, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "706406cc-3c00-4906-b7a6-0c7ba82c7e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "img_height = 100\n",
    "img_width = 100"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96941f6f-cceb-413b-9ef0-9169917a37b6",
   "metadata": {},
   "source": [
    "Create a data generator called train_data_gen using .flow_from_directory() and specify the batch size, the path to the training folder, shuffle=True, the target size as (img_height, img_width), and the class mode as binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f3fb98-255a-49c3-82d1-b3da65771dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size, directory=train_dir, \n",
    "                                                          shuffle=True, target_size=(img_height, img_width), \n",
    "                                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38930f3c-74a6-40a0-b70e-0977d950b742",
   "metadata": {},
   "source": [
    "Create a data generator called val_data_gen using .flow_from_directory() and specify the batch size, paths to the validation folder, shuffle=True, the target size as (img_height, img_width), and the class mode as binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90d3a196-61b6-4d19-9a85-9231c5e292f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size, directory=validation_dir, \n",
    "                                                             shuffle=True, target_size=(img_height, img_width), \n",
    "                                                             class_mode='binary')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9823859c-7e85-4b51-be22-c97db3c82600",
   "metadata": {},
   "source": [
    "Import numpy as np, tensorflow as tf, and layers from tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "849ce09d-489b-488a-9723-a9b959bff90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a1dc714-d059-49f6-9082-0efe5c0d6982",
   "metadata": {},
   "source": [
    "Set 8 (this is totally arbitrary) as the seed for numpy and tensorflow using np.random_seed() and tf.random.set_seed(), respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e55f6275-e07f-455e-a213-8d3c06b009e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cfcd60b-680a-4fb4-87a3-6a619d7348dc",
   "metadata": {},
   "source": [
    "Instantiate a tf.keras.Sequential() class into a variable called model with the following layers: A convolution layer with 64 kernels of shape 3, ReLU as the activation function, and the required input dimensions; a max pooling layer; a convolution layer with 128 kernels of shape 3 and ReLU as the activation function; a max pooling layer; a flatten layer; a fully connected layer with 128 units and ReLU as the activation function; a fully connected layer with 1 unit and sigmoid as the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10e23f9b-7bc8-4b65-a0dd-a668bf63e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([layers.Conv2D(64, (3), activation='relu', input_shape=(img_height, img_width, 3)), # A convolution layer with 64 kernels of shape 3, ReLU as the activation function, and the required input dimensions\n",
    "                            layers.MaxPool2D(), # a max pooling layer\n",
    "                            layers.Conv2D(128, (3), activation='relu'), # a convolution layer with 128 kernels of shape 3 and ReLU as the activation function\n",
    "                            layers.MaxPool2D(), # a max pooling layer\n",
    "                            layers.Flatten(), # a flatten layer\n",
    "                            layers.Dense(128, activation='relu'), # a fully connected layer with 128 units and ReLU as the activation function\n",
    "                            layers.Dense(1, activation='sigmoid')]) # a fully connected layer with 1 unit and sigmoid as the activation function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92b3a35d-85e7-4397-b01e-29f2395ba7ba",
   "metadata": {},
   "source": [
    "Instantiate a tf.keras.optimizers.Adam() class with 0.001 as the learning rate and save it to a variable called optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b950a201-6b95-4924-b3bc-10cb8f5c58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf173493-3498-41fd-b44c-cfeded35c792",
   "metadata": {},
   "source": [
    "Compile the neural network using .compile() with loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ee48321-36a7-44a3-8b5d-9e2aa92cb320",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d68f71a-a4a7-4215-8f54-a5dc16a51ce0",
   "metadata": {},
   "source": [
    "Print a summary of the model using .summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8e2f898-0ba0-4d3f-bfbf-e44e9f8d0afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 98, 98, 64)        1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 49, 49, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 47, 47, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 23, 23, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 67712)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8667264   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,743,041\n",
      "Trainable params: 8,743,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0c344b9-e633-469e-a788-2c2251c0f125",
   "metadata": {},
   "source": [
    "The preceding summary shows us that there are more than 8,700,000 parameters to be optimized with this model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c47424f-7734-4ec4-a57f-68375fae2b83",
   "metadata": {},
   "source": [
    "Fit the neural networks with fit_generator() and provide the train and validation data generators, epochs=5, the steps per epoch, and the validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ed747e0-184f-484a-a2c3-c601096f8cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.7337 - accuracy: 0.5410 - val_loss: 0.6845 - val_accuracy: 0.5222\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.6530 - accuracy: 0.6360 - val_loss: 0.6351 - val_accuracy: 0.6512\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.5713 - accuracy: 0.6990 - val_loss: 0.6668 - val_accuracy: 0.6240\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.4585 - accuracy: 0.7825 - val_loss: 0.6537 - val_accuracy: 0.7006\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.3174 - accuracy: 0.8575 - val_loss: 0.7862 - val_accuracy: 0.6623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa7412d3460>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data_gen, steps_per_epoch=total_train // batch_size, epochs=5, validation_data=val_data_gen, \n",
    "                   validation_steps=total_val // batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07548530-61fc-4ca0-9878-bf858deff982",
   "metadata": {},
   "source": [
    "We've trained our CNN for five epochs and achieved an accuracy score of 0.86 for the training set, and 0.6623 for the validation set. Our model is overfitting quite a lot. You may want to try the training with different architectures to see whether you can improve this score and reduce overfitting. You can also try feeding this model with some images of cats or dogs of your choice and see the output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56c3763b-46b1-473e-aaa8-bad679fa5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce overfitting adding another full layer with 64 units\n",
    "model2 = tf.keras.Sequential([layers.Conv2D(64, (3), activation='relu', input_shape=(img_height, img_width, 3)), # A convolution layer with 64 kernels of shape 3, ReLU as the activation function, and the required input dimensions\n",
    "                            layers.MaxPool2D(), # a max pooling layer\n",
    "                            layers.Conv2D(128, (3), activation='relu'), # a convolution layer with 128 kernels of shape 3 and ReLU as the activation function\n",
    "                            layers.MaxPool2D(), # a max pooling layer\n",
    "                            layers.Flatten(), # a flatten layer\n",
    "                            layers.Dense(128, activation='relu'), # a fully connected layer with 128 units and ReLU as the activation function\n",
    "                            layers.Dense(64, activation='relu'), # a fully connected layer with 64 units and ReLU as the activation function\n",
    "                            layers.Dense(1, activation='sigmoid')]) # a fully connected layer with 1 unit and sigmoid as the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4a5b755-82eb-4d0d-978f-36f39a18d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "857ed9f0-87bf-44eb-84aa-3ad66d82e4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 98, 98, 64)        1792      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 49, 49, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 47, 47, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 23, 23, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 67712)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               8667264   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,751,233\n",
      "Trainable params: 8,751,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8a7274a-5ca7-4c0a-b3de-afa93bd0ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/125 [==============================] - 24s 185ms/step - loss: 0.7609 - accuracy: 0.4770 - val_loss: 0.6931 - val_accuracy: 0.5010\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.6934 - accuracy: 0.4830 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.6935 - accuracy: 0.4990 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.6930 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa7498c9640>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_data_gen, steps_per_epoch=total_train // batch_size, epochs=5, validation_data=val_data_gen, \n",
    "                   validation_steps=total_val // batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e023628-7fdf-4502-8edc-2e2b8d041565",
   "metadata": {},
   "source": [
    "Model is no longer overfitting but the accuracy has suffered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82c0b0-f1fc-46b3-bfa3-ad9e47533acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
