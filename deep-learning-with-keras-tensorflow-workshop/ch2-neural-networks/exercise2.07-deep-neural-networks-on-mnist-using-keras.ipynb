{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cf3dd217-9162-40c3-b7b4-0422113af604",
   "metadata": {},
   "source": [
    "In this exercise, we will perform a multiclass classification by implementing a deep neural network (multi-layer) for the MNIST dataset where our input layer comprises 28 × 28 pixel images flattened to 784 input nodes followed by 2 hidden layers, the first with 50 neurons and the second with 20 neurons. Lastly, there will be a Softmax layer consisting of 10 neurons since we are classifying the handwritten digits into 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d12ff15-1e13-4229-a12f-92fd04e34b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import Keras libraries\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938c62e0-3cc6-4f56-bfd2-85e32cf1b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_features, train_labels), (test_features, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12db334c-0953-4bc2-9c74-67ca6319975e",
   "metadata": {},
   "source": [
    "train_features has the training images in the form of 28 x 28 pixel values.\n",
    "\n",
    "train_labels has the training labels. Similarly, test_features has the test images in the form of 28 x 28 pixel values. test_labels has the test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d318ce69-1365-4900-a31a-1c37585004fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "train_features, test_features = train_features/255.0, test_features/255.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e951766-4fca-4f17-8ab4-da382e072248",
   "metadata": {},
   "source": [
    "The pixel values of the images range from 0-255. We need to normalize the values by dividing them by 255 so that the range goes from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a5d0e9-5349-4554-baab-1ffd6dd2d4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 05:54:35.992876: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# build the sequential model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28,28)))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dense(units=20, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd4b82f0-8865-49f5-8c3a-8b45109be6b7",
   "metadata": {},
   "source": [
    "There are couple of points to note. The first layer in this case is not actually a layer of neurons but a Flatten function. This flattens the 28 x 28 image into a single array of 784, which is fed to the first hidden layer of 50 neurons. The last layer has 10 neurons corresponding to the 10 classes with a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ed19e97-52a0-47c5-965a-a07965305b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide training parameters\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfcee3be-ffe3-47cc-8a50-60c6a3270f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                39250     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1020      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,480\n",
      "Trainable params: 40,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62b7fd2d-f1a0-4b17-88cd-debbe092c30b",
   "metadata": {},
   "source": [
    "The loss function used here is different from the binary classifier. For a multiclass classifier, the following loss functions are used: sparse_categorical_crossentropy, which is used when the labels are not one-hot encoded, as in this case; and, categorical_crossentropy, which is used when the labels are one-hot encoded."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c07cde9f-fca8-493e-9c5a-8f135567992a",
   "metadata": {},
   "source": [
    "In the model summary, we can see that there are a total of 40,480 parameters—weights and biases—to learn across the hidden layers to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e295927e-e018-4620-b2d9-c1c2305a13b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 2s 943us/step - loss: 0.3441 - accuracy: 0.8998\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 2s 926us/step - loss: 0.1604 - accuracy: 0.9522\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 2s 921us/step - loss: 0.1162 - accuracy: 0.9650\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 2s 918us/step - loss: 0.0934 - accuracy: 0.9714\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 2s 917us/step - loss: 0.0775 - accuracy: 0.9760\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 2s 897us/step - loss: 0.0672 - accuracy: 0.9789\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 2s 883us/step - loss: 0.0581 - accuracy: 0.9811\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 2s 885us/step - loss: 0.0512 - accuracy: 0.9838\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 2s 889us/step - loss: 0.0458 - accuracy: 0.9855\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 2s 880us/step - loss: 0.0410 - accuracy: 0.9870\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 2s 880us/step - loss: 0.0370 - accuracy: 0.9880\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 2s 926us/step - loss: 0.0335 - accuracy: 0.9890\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 2s 916us/step - loss: 0.0301 - accuracy: 0.9901\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 2s 911us/step - loss: 0.0278 - accuracy: 0.9909\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 2s 909us/step - loss: 0.0251 - accuracy: 0.9917\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 2s 920us/step - loss: 0.0236 - accuracy: 0.9924\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 2s 921us/step - loss: 0.0213 - accuracy: 0.9929\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 2s 931us/step - loss: 0.0221 - accuracy: 0.9930\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 2s 934us/step - loss: 0.0179 - accuracy: 0.9939\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 2s 927us/step - loss: 0.0178 - accuracy: 0.9940\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 2s 926us/step - loss: 0.0170 - accuracy: 0.9941\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 2s 926us/step - loss: 0.0161 - accuracy: 0.9948\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 2s 922us/step - loss: 0.0140 - accuracy: 0.9949\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 2s 930us/step - loss: 0.0143 - accuracy: 0.9951\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 2s 910us/step - loss: 0.0129 - accuracy: 0.9956\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 2s 904us/step - loss: 0.0125 - accuracy: 0.9958\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 2s 904us/step - loss: 0.0116 - accuracy: 0.9962\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 2s 902us/step - loss: 0.0120 - accuracy: 0.9957\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 2s 899us/step - loss: 0.0109 - accuracy: 0.9963\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 2s 906us/step - loss: 0.0108 - accuracy: 0.9960\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 2s 892us/step - loss: 0.0111 - accuracy: 0.9960\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 2s 886us/step - loss: 0.0100 - accuracy: 0.9968\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 2s 888us/step - loss: 0.0110 - accuracy: 0.9965\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 2s 885us/step - loss: 0.0083 - accuracy: 0.9969\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 2s 899us/step - loss: 0.0097 - accuracy: 0.9966\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 2s 896us/step - loss: 0.0089 - accuracy: 0.9968\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 2s 886us/step - loss: 0.0088 - accuracy: 0.9971\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 2s 888us/step - loss: 0.0089 - accuracy: 0.9971\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 2s 882us/step - loss: 0.0100 - accuracy: 0.9969\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 2s 881us/step - loss: 0.0080 - accuracy: 0.9973\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 2s 887us/step - loss: 0.0097 - accuracy: 0.9967\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 2s 902us/step - loss: 0.0086 - accuracy: 0.9973\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 2s 886us/step - loss: 0.0090 - accuracy: 0.9971\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 2s 893us/step - loss: 0.0077 - accuracy: 0.9976\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 2s 898us/step - loss: 0.0086 - accuracy: 0.9973\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 2s 895us/step - loss: 0.0085 - accuracy: 0.9974\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 2s 882us/step - loss: 0.0076 - accuracy: 0.9976\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 2s 893us/step - loss: 0.0083 - accuracy: 0.9975\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 2s 873us/step - loss: 0.0074 - accuracy: 0.9977\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 2s 872us/step - loss: 0.0075 - accuracy: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb28d142b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(train_features, train_labels, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e38c3b-5615-459b-a3a1-5550e773f86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 672us/step - loss: 0.2242 - accuracy: 0.9715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22421348094940186, 0.9714999794960022]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model by calling evaluate()\n",
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9699e7e9-bee5-4178-8dca-cd74453d51c5",
   "metadata": {},
   "source": [
    "Now that the model is trained and tested, in the next few steps, we will run the prediction with some images selected randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79e944a7-596f-4baf-bf9b-70461444e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a random image from test dataset; 200th image\n",
    "loc = 200\n",
    "test_image = test_features[loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4637cbb-bb2e-4d08-af0c-c3f8fb057aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view shape of image\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0650dbd6-97c3-4194-9cdd-ebbcffa5af07",
   "metadata": {},
   "source": [
    "We can see that the shape of the image is 28 x 28. However, the model expects 3-dimensional input. We need to reshape the image accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e15defe4-7000-46ef-b669-9799e20ad5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape image\n",
    "test_image = test_image.reshape(1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7ed7cc6-5484-406c-8f55-8fffa0a0d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3243749e-23 9.2226690e-22 3.5045666e-10 9.9999976e-01 0.0000000e+00\n",
      "  4.7475019e-08 9.7370665e-37 3.8477758e-16 2.3680171e-07 4.0813560e-22]]\n"
     ]
    }
   ],
   "source": [
    "# call predict() and store output in variable called result\n",
    "result = model.predict(test_image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e01985fe-b4d8-4735-ba39-6adb99cc89dc",
   "metadata": {},
   "source": [
    "result has the output in the form of 10 probability values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bc04598-5f1b-4241-8b97-d581cefb0e48",
   "metadata": {},
   "source": [
    "The position of the highest value will be the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d4de5d0-0328-409f-b432-d025049c4532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use argmax() to find out prediction\n",
    "result.argmax()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45550eb0-a229-465f-8081-a1945a6f2759",
   "metadata": {},
   "source": [
    "In order to check whether the prediction is correct, we check the label of the corresponding image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e830aae-d90e-45f3-b29e-15c01337e721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[loc]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc5ca9dd-0b9d-49e7-9d52-03361f59fc77",
   "metadata": {},
   "source": [
    "We can also visualize the image using pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42d00e4d-8d99-4db7-812f-cc1f8330bd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbb0a711c70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_features[loc])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "606167d1-382e-4397-8c05-2d1ea3920b08",
   "metadata": {},
   "source": [
    "And this shows that the prediction is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573a53d-3f65-4626-8ca2-ab899a566c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
