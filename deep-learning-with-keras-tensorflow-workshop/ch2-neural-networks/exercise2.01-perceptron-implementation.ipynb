{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7020bdbc-d756-4baf-bd53-129c4356a7e8",
   "metadata": {},
   "source": [
    "In this exercise we will implement the perceptron in TensorFlow for an OR table\n",
    "z = tf.add(tf.matmul(X, W), B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379971d2-451f-4eb9-a095-2c9dc9b092a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package(s)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32cb6043-dfa2-4d80-ba65-6b729b1d1646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-14 08:38:33.798990: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# set the input data and labels of the OR table in TensorFlow\n",
    "X = tf.Variable([[0.,0.],\n",
    "                 [0.,1.],\n",
    "                 [1.,0.],\n",
    "                 [1.,1.]], \n",
    "                 dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "768c513f-9134-4867-85b1-105271aa9728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(4, 2) dtype=float32, numpy=\n",
      "array([[0., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70c121a7-2145-4ca5-a3a3-586dcd6e3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Set actual labels in TensorFlow and use reshape() on y vector into a 4x1 matrix\n",
    "y = tf.Variable([0,1,1,1], dtype=tf.float32)\n",
    "y = tf.reshape(y, [4,1])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d35a9050-e3ca-458e-8e65-7dfa7c7148da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97b099eb-0794-475a-b6c1-166ce3af808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design parameters of a perceptron\n",
    "\"\"\"\n",
    "Number of neurons (units) = 1\n",
    "Number of features (inputs) = 2 (number of examples x number of features)\n",
    "\"\"\"\n",
    "\n",
    "# activation function will be sigmoid function since we are doing binary classification\n",
    "NUM_FEATURES = X.shape[1]\n",
    "OUTPUT_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f8b15ea-87e6-4ee8-977f-a27ad8b509dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[0.],\n",
      "       [0.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# define the connections weight matrix in TensorFlow\n",
    "W = tf.Variable(tf.zeros([NUM_FEATURES, OUTPUT_SIZE]),dtype=tf.float32)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c20bc217-a042-47ff-99c0-2df660985a1b",
   "metadata": {},
   "source": [
    "The weight matrix would essentially be a columnar matrix as shown in the following figure. It will have the following dimension: number of features (columns) × output size. The output size will be dependent on the number of neurons—in this case, it is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07164d1d-4612-47b6-aef6-0a0ce83d96cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[0.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# variable for bias\n",
    "B = tf.Variable(tf.zeros([OUTPUT_SIZE, 1]), dtype=tf.float32)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a903fab-0129-426b-9f71-74f562d495af",
   "metadata": {},
   "source": [
    "There is only one bias per neuron, so in this case, the bias is just one number in the form of a single-element array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93cb7926-9acf-4a28-bd51-40d9f18a3e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# define function called perceptron to get the output\n",
    "def perceptron(X):\n",
    "    z = tf.add(tf.matmul(X, W), B)\n",
    "    output = tf.sigmoid(z)\n",
    "    return output\n",
    "print(perceptron(X))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0593d17e-d9ef-4fb9-bb2b-a38f1547f038",
   "metadata": {},
   "source": [
    "As we can see, the predictions are not quite accurate. We will learn how to improve the results in the sections that follow.\n",
    "Keep in mind that it is just the implementation of the model; we have not done any training. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "07e2845f-5957-4577-b15e-ae5eb5f61780",
   "metadata": {},
   "source": [
    "In the previous section, we covered most of the preceding components: the data representation of the input data and the true labels in TensorFlow. For layers, we have the linear layer and the activation functions, which we saw in the form of the net input function and the sigmoid function respectively. For the neural network representation, we made a function called perceptron(), which uses a linear layer and a sigmoid layer to perform predictions. What we did in the previous section using input data and initial weights and biases is called forward propagation. \n",
    "\n",
    "The actual neural network training involves two stages: forward propagation and backward propagation. We will explore them in detail in the next few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ceba3e9-f3b9-4508-a850-18d90ff224f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]] [[0]]\n"
     ]
    }
   ],
   "source": [
    "tf.print(W,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1937b9-d3fc-4da2-8400-7d92abb98e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
