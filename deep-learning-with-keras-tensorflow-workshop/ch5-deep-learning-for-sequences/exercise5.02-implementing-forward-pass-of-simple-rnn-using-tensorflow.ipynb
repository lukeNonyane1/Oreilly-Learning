{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111b40c2-4c6d-4ba0-8776-93dfabf54899",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exercise 5.02: Implementing the Forward Pass of a Simple RNN Using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e2cd6e1-24cb-4ced-adef-58151eb0e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f355e9bd-4577-452e-b26f-9f2983957e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2 # number of inputs\n",
    "num_neurons = 3 # number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30b849-6484-436c-99f4-7784252cf8f3",
   "metadata": {},
   "source": [
    "We will have two inputs at each time step. Let's call them xt0 and xt1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a8751-9089-450d-83f6-df07dc7ce1f6",
   "metadata": {},
   "source": [
    "Define the variables for the weight matrices. We need two of them – one for the feedforward weights and another for the recurrent weights. Initialize them randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13cb18c-675f-408c-b1ba-1d485d9f406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 06:13:56.732956: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "Wf = tf.Variable(\n",
    "    tf.random.normal(shape=[num_inputs, num_neurons]) # 2,3\n",
    ")\n",
    "\n",
    "Wr = tf.Variable(\n",
    "    tf.random.normal(shape=[num_neurons, num_neurons]) # 3,3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9005d30-54d5-4015-b4e2-927565216424",
   "metadata": {},
   "source": [
    "Notice the dimensions for the recurrent weights – it is a square matrix, with as many rows/columns as the number of neurons in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391b06a-ef24-47e8-9cc0-f5f0da921f53",
   "metadata": {},
   "source": [
    "Add the bias variable (to make the activations fit the data better), with as many zeros as the number of neurons in the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64cf328d-08ba-4003-92ae-b263dc8521b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable(\n",
    "    tf.zeros([1,num_neurons])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a2b8c9-88b7-4bac-8a21-0c4945d87cea",
   "metadata": {},
   "source": [
    "Create the data – three examples for xt0 (two inputs, three examples) as [[0,1], [2,3], [4,5]] and xt1 as [[100,101], [102,103], [104,105]] – as numpy arrays of the float32 type (consistent with dtype for TensorFlow's default float representation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36873af2-6378-4319-8dff-204e3e8784cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt0_batch = np.array(\n",
    "    [\n",
    "        [0,1],\n",
    "        [2,3],\n",
    "        [4,5]\n",
    "    ]\n",
    ").astype(np.float32)\n",
    "\n",
    "xt1_batch = np.array(\n",
    "    [\n",
    "        [100,101],\n",
    "        [102,103],\n",
    "        [104,105]\n",
    "    ]\n",
    ").astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de955239-720a-4beb-82bf-118e1d0d837d",
   "metadata": {},
   "source": [
    "Define a function named forward_pass to apply a forward pass to the given data, that is, xt0, xt1. Use tanh as the activation function. The output at t=0 should be derived from Wf and xt0 alone. The output at t=1 must use yt0 with the recurrent weights, Wf, and use the new input, xt1. The function should return outputs at the two time steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a3a8178-7bb2-418c-863b-78747a534526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(xt0, xt1):\n",
    "    yt0 = tf.tanh(tf.matmul(xt0, Wf) + b)\n",
    "    yt1 = tf.tanh(tf.matmul(yt0, Wr) + tf.matmul(xt1, Wf) + b)\n",
    "    return yt0, yt1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01ea58-2552-4412-abb3-f776868d4c63",
   "metadata": {},
   "source": [
    "Note that there is no recurrent weight here at time step 0; it comes into play only after the first time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40a577-d2e7-41ca-80c9-e186f8cdd1eb",
   "metadata": {},
   "source": [
    "Perform the forward pass by calling the forward_pass function with the created data (xt0_batch, xt1_batch) and put the output into variables, yt0_output and yt1_output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04b9efbd-839a-4a1b-a185-b64eb20ca800",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt0_output, yt1_output = forward_pass(xt0_batch, xt1_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6201261-6916-40a0-a14d-e0d0959b69b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.776318431 -0.844548464 0.438419849]\n",
      " [-0.0857750699 -0.993522227 0.516408086]\n",
      " [0.698345721 -0.999749422 0.586677969]]\n"
     ]
    }
   ],
   "source": [
    "tf.print(yt0_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "399a48dd-fec3-4917-946e-dd9f6a06d250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 -1 0.999998629]\n",
      " [1 -1 0.999998331]\n",
      " [1 -1 0.999997377]]\n"
     ]
    }
   ],
   "source": [
    "tf.print(yt1_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76be9fc-6922-421a-be2d-952ce6eaf92c",
   "metadata": {},
   "source": [
    "We can see that the final output at time t=1 is a 3x3 matrix – representing the outputs for the three neurons in the hidden layer for the three instances of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
