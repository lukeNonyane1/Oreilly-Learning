{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed824be-3789-4783-91b9-8cb09bcdd320",
   "metadata": {},
   "source": [
    "### In this chapter, we will use plain RNNs and variants of RNNs on a sentiment classification task: processing the input sequence and predicting whether the sentiment is positive or negative.\n",
    "\n",
    "We'll use the IMDb reviews dataset for this task. The dataset contains 50,000 movie reviews, along with their sentiment â€“ 25,000 highly polar movie reviews for training and 25,000 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb299aa-8cb7-4af4-bce7-cb47b75a8a11",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd38d28d-d9ca-48b7-a63c-c9c2558fbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd1b59-ebd7-4391-b504-dca24094657f",
   "metadata": {},
   "source": [
    "With the module imported, importing the dataset (tokenized and separated into train and test sets) is as easy as running imdb.load_data. The only parameter we need to provide is the vocabulary size we wish to use.\n",
    "\n",
    "Here, we will specify a vocabulary size of 8,000 for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbd4bbb-cb12-411d-a300-6ce5272b00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8945d3-f9f3-431c-8bba-c98e18eed69e",
   "metadata": {},
   "source": [
    "Let's inspect the X_train variable to see what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171d1f18-9f11-48da-9fff-5a93463d3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "[1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 2, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 2, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(x_train[5]))\n",
    "print(x_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded045e-970a-41a6-8ba5-1c5144816732",
   "metadata": {},
   "source": [
    "The next step is to define an upper limit on the length of the sequences that we'll work with and limit all sequences to the defined maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b28971f-1895-4221-8c78-ba42a62409cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b4ecf-cd10-49f7-8137-66ad6ad44af8",
   "metadata": {},
   "source": [
    "The next step is to get all our sequences to the same length using the pad_sequences utility from Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d165980a-7d64-4dc5-90c5-e4c34cd16a81",
   "metadata": {},
   "source": [
    "#### Staging and Preprocessing Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a58fa-a443-44a8-8e13-f390af90ccb5",
   "metadata": {},
   "source": [
    "The pad_sequences utility from the sequences module in Keras helps us in getting all the sequences to a specified length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ae9027-a9b0-4f7b-8a35-1d9bec9b0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2756b352-bf4c-4e52-8420-6d74a8205ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374a10b4-9870-4001-946a-2106aa30f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    1  778  128   74   12  630  163   15    4 1766 7982\n",
      " 1051    2   32   85  156   45   40  148  139  121  664  665   10   10\n",
      " 1361  173    4  749    2   16 3804    8    4  226   65   12   43  127\n",
      "   24    2   10   10]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879314a9-b214-4639-8857-297e781545a2",
   "metadata": {},
   "source": [
    "We can see that there are plenty of 0s at the beginning of the result. As you may have inferred, this is the padding that's done by the pad_sequence utility because the input sequence was shorter than 200. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298b795-a94b-4db5-b254-2cbc161a4efc",
   "metadata": {},
   "source": [
    "#### The Embedding Layer\n",
    "The embedding layer is always the first layer in the model. You can follow it up with any architecture of your choice (RNNs, in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb0ebc-c5c0-43ec-b37b-cab92ac99f90",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Building the Plain RNN Model\n",
    "#### Exercise 6.01: Building and Training an RNN Model for Sentiment Classification\n",
    "In this exercise, we will build and train an RNN model for sentiment classification. Initially, we will define the architecture for the recurrent and prediction layers, and we will assess the model's performance on the test data. We will add the embedding layer and some dropout and complete the model definition by adding the RNN layer, dropout, and a dense layer to finish. Then, we'll check the accuracy of the predictions on the test data to assess how well the model generalizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e80bd6-8b47-4a88-ad9b-e8520bca760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for numpy and tensforflow for reproducible results\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439b6dc-a7a4-4f9b-b28b-15cc02b81d45",
   "metadata": {},
   "source": [
    "import all the necessary packages and layers and initializing a sequential model named model_rnn using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d259c20-4d64-41bd-a84f-12ff2210f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 07:35:48.920851: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Flatten, Dense, Embedding, SpatialDropout1D, Dropout\n",
    "model_rnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21cb16e8-8998-4864-be38-40a519aff79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer\n",
    "model_rnn.add(Embedding(vocab_size, output_dim=32))\n",
    "model_rnn.add(SpatialDropout1D(0.4)) # miniize overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13bb786b-89ee-486f-a750-1d1e5752d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleRNN layer\n",
    "model_rnn.add(SimpleRNN(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7e1d34a-1fed-4ba3-8a81-adb0681505c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout layer\n",
    "model_rnn.add(Dropout(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e4018b-6fdc-4786-b734-c2a3f2e59871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction layer (Dense)\n",
    "model_rnn.add(Dense(1, activation='sigmoid')) # sigmoid because we have binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83b9d53b-6920-4c26-bb2f-174233209275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          256000    \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, None, 32)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 258,113\n",
      "Trainable params: 258,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile the model and view summary\n",
    "model_rnn.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd38f8-53a7-4acf-9dbc-b55b8dcdf312",
   "metadata": {},
   "source": [
    "We can see that there are 258,113 parameters, most of which are present in the embedding layer. The reason for this is that the word embeddings are being learned during the training â€“ so we're learning the embedding matrix, which is of dimensionality vocab_size(8000) Ã— output_dim(32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6741778f-a37c-4a0e-ba94-c6233c75d311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6123 - accuracy: 0.6627 - val_loss: 0.4799 - val_accuracy: 0.7924\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.4404 - accuracy: 0.8126 - val_loss: 0.3820 - val_accuracy: 0.8378\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.3573 - accuracy: 0.8559 - val_loss: 0.3762 - val_accuracy: 0.8476\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.3201 - accuracy: 0.8755 - val_loss: 0.4821 - val_accuracy: 0.8222\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.2886 - accuracy: 0.8906 - val_loss: 0.3584 - val_accuracy: 0.8604\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.2703 - accuracy: 0.8976 - val_loss: 0.3396 - val_accuracy: 0.8714\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.2479 - accuracy: 0.9064 - val_loss: 0.6179 - val_accuracy: 0.8018\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.2319 - accuracy: 0.9123 - val_loss: 0.3673 - val_accuracy: 0.8632\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.2118 - accuracy: 0.9218 - val_loss: 0.3658 - val_accuracy: 0.8520\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.1940 - accuracy: 0.9281 - val_loss: 0.4071 - val_accuracy: 0.8262\n"
     ]
    }
   ],
   "source": [
    "# fit model on train data\n",
    "history_rnn = model_rnn.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2, # gives us a sense of the model performance on unseen data.\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91252f81-d356-4729-8466-d79e71109dcb",
   "metadata": {},
   "source": [
    "From the training output, we can see that the validation accuracy goes up to about 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "966a2156-6331-48b9-96af-183d1d653c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83172\n"
     ]
    }
   ],
   "source": [
    "# prediction on test data\n",
    "y_test_pred = (model_rnn.predict(x_test) > 0.5).astype('int32') # use this when working on binary classification.\n",
    "classes_x = np.argmax(y_test_pred, axis=1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d032a5-ec23-4354-917e-9e33204266e5",
   "metadata": {},
   "source": [
    "Overcome deprecated Sequential.predict_classes() method.\n",
    "\n",
    "Multi-class classification - Softmax last layer\n",
    "- np.argmax(model.predict(x), axis=-1)\n",
    "\n",
    "Binary classification - Sigmoid last layer\n",
    "- (model.predict(x) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00413695-5bb5-4fa3-aa98-6f79f029ab05",
   "metadata": {},
   "source": [
    "We can see that the model does a decent job. We used a simple architecture with 32 neurons and used a vocabulary size of just 8000. Tweaking these and other hyperparameters may get you better results and you are encouraged to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f999f45-713c-4575-8d70-c3f6986b8e71",
   "metadata": {},
   "source": [
    "In this exercise, we have seen how to build an RNN-based model for text. We saw how an embedding layer can be used to derive word vectors for the task at hand. These word vectors are the representations for each incoming term, which are passed to the RNN layer. We have seen that even a simple architecture can give us good results. Now, let's discuss how this model can be used to make predictions on new, unseen reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee96e2-916d-4f1d-baa9-eb930f5a0437",
   "metadata": {},
   "source": [
    "#### Making Predictions on Unseen Data\n",
    "Our model (model_rnn) was trained on IMDb reviews that were tokenized, had their case lowered, had punctuation removed, had a defined vocabulary size, and were converted into a sequence of indices. Our function/pipeline for preparing data for the RNN model needs to perform the same steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa37c87e-45bf-4bd7-855b-0c94cb437296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable containing raw review text\n",
    "inp_review = 'An excellent movie!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f7d237-615c-4cae-b778-5290e64960af",
   "metadata": {},
   "source": [
    "The sentiment in the text is positive. If the model is working well enough, it should predict the sentiment as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd0b720e-6dd1-4d7c-840a-bb566750facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d3f3f-cf15-4b07-a8db-b967b9e9cb29",
   "metadata": {},
   "source": [
    "The code above must tokenize this text into its constituent terms, normalize its case, and remove punctuation.\n",
    "\n",
    "Check if it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c7fc618-beab-4550-9d1e-4e689ff25fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'excellent', 'movie']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(inp_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8689b5ef-71cf-4178-9300-de5b03096c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vacabulary into dictionary named word_map\n",
    "word_map = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a8a20b8-612f-49c8-a0bf-0787160ddee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the mapping to 8000 terms by sorting word_map variable on index and picking the first 8000 terms to match what is used on the train data.\n",
    "vocab_map = dict(sorted(word_map.items(), key=lambda x: x[1])[:vocab_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae412d9-156e-46ab-b34f-9bc5679f4004",
   "metadata": {},
   "source": [
    "The vocab map will be a dictionary containing the term for index mapping for the 8000 terms in the vocabulary. Using this mapping, we'll convert the tokenized sentence into a sequence of term indices by performing a lookup for each term and returning the corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11a88c56-7cea-434b-8683-798a84632983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess function that accepts raw text, applies the text_to_word_sequence utility to it, performs a lookup from vocab_map, and returns the corresponding sequence of integers\n",
    "def preprocess(review):\n",
    "    inp_tokens = text_to_word_sequence(review)\n",
    "    seq = []\n",
    "    for token in inp_tokens:\n",
    "        seq.append(vocab_map.get(token))\n",
    "        \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ace01179-b0ec-45ac-b6a1-e6acda576730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 318, 17]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(inp_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d6152-5577-40a1-b8e3-df62df66662d",
   "metadata": {},
   "source": [
    "This is the sequence of term indices corresponding to the raw text. Note that the data is now in the same format as the IMDb data we loaded. This sequence of indices can be fed to the RNN model (using the predict_classes method) to classify the sentiment, as shown in the following code. If the model is working well enough, it should predict the sentiment as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cb17aab-d23d-4498-a123-6965786eef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_rnn.predict([preprocess(inp_review)]) > 0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530650a-3303-409e-9217-6d1b8f295ae0",
   "metadata": {},
   "source": [
    "The output prediction is 1 (positive), just as we expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607dea26-bd79-454c-8c8c-cc86fda8b42a",
   "metadata": {},
   "source": [
    "Let's apply the function to another raw text review and supply it to the model for prediction. Let's update the inp_review variable so that it contains the text \"Don't watch this movie â€“ poor acting, poor script, bad direction.\" The sentiment in the review is negative. We expect the model to classify it as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95a03414-0224-4eb9-8b98-5cd179259075",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_review = \"Don't watch this movie - poor acting, poor script, bad direction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69410297-526c-41db-8b71-aae495f92176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_rnn.predict([preprocess(inp_review)]) > 0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd910ea7-09d2-4fc7-9183-aa3700a3964e",
   "metadata": {},
   "source": [
    "The predicted sentiment is negative, just as we would expect the model to behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0caae-115b-4335-b7d6-3fa35b57dc48",
   "metadata": {},
   "source": [
    "#### LSTMs, GRUs, and Other Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe50305-32e7-4c90-b27a-6c395cf9c385",
   "metadata": {},
   "source": [
    "#### Exercise 6.02: LSTM-Based Sentiment Classification Model\n",
    "In this exercise, we will build a simple LSTM-based model to predict sentiment on our data. We will continue with the same setup we used previously (that is, the number of cells, embedding dimensions, dropout, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bc38761-5bd7-42c4-a0b4-59506696d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e55307-2856-4658-987f-4afb42073272",
   "metadata": {},
   "source": [
    "Instantiate the sequential model, add the embedding layer with the appropriate dimensions, and add a 40% spatial dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3dd9cd8-88f3-4f44-87e6-f803bbf9b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, output_dim=32))\n",
    "model_lstm.add(SpatialDropout1D(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f61bf3b5-c9d8-4f4d-951f-a98605ee8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "model_lstm.add(LSTM(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6916e17c-38dc-410b-88a9-b49e95b9d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 32)          256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, None, 32)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264,353\n",
      "Trainable params: 264,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# dropout and dense layers\n",
    "model_lstm.add(Dropout(0.4))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',metrics=['accuracy']\n",
    ")\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a61c91-77fa-4b81-bed6-63dbe7f3017b",
   "metadata": {},
   "source": [
    "We can see from the model summary that the number of parameters in the LSTM layer is 8320. A quick check can confirm that this is exactly four times the number of parameters in the plain RNN layer we saw in Exercise 6.01, Building and Training an RNN Model for Sentiment Classification, which is in line with our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfc79899-9312-4c54-a228-970cb4b6b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "157/157 [==============================] - 12s 65ms/step - loss: 0.5299 - accuracy: 0.7315 - val_loss: 0.4013 - val_accuracy: 0.8332\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.3298 - accuracy: 0.8709 - val_loss: 0.3489 - val_accuracy: 0.8568\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.2689 - accuracy: 0.8972 - val_loss: 0.2847 - val_accuracy: 0.8798\n",
      "Epoch 4/5\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.2397 - accuracy: 0.9112 - val_loss: 0.3224 - val_accuracy: 0.8832\n",
      "Epoch 5/5\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.2163 - accuracy: 0.9187 - val_loss: 0.2984 - val_accuracy: 0.8850\n"
     ]
    }
   ],
   "source": [
    "# fit on training data for 5 epochs and batch size of 128\n",
    "history_lstm = model_lstm.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29b1747a-e02c-4424-8363-b6e1a5e51360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87288\n"
     ]
    }
   ],
   "source": [
    "# test data performance\n",
    "y_test_pred_lstm = (model_lstm.predict(x_test) > 0.5).astype(\"int32\")\n",
    "print(accuracy_score(y_test, y_test_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ddc6fd-2ca1-4190-89c2-ad991b6b6112",
   "metadata": {},
   "source": [
    "The accuracy we got (87%) is a significant improvement from the accuracy we got using plain RNNs (83.2%). It looks like the extra parameters and the extra predictive power from the cell state came in handy for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8124698-340d-43ca-8240-42c2016866b2",
   "metadata": {},
   "source": [
    "#### Exercise 6.03: GRU-Based Sentiment Classification Model\n",
    "In this exercise, we will build a simple GRU-based model to predict sentiments in our data. We will continue with the same setup that we used previously (that is, the number of cells, embedding dimensions, dropout, and so on). Using GRUs instead of LSTMs in the model is as simple as replacing \"LSTM\" with \"GRU\" when adding the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e1f702a-b51d-4fa9-948c-44d98ea7c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GRU layer\n",
    "from tensorflow.keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f54b5180-af81-4358-b47d-5b0a0ebb0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru= Sequential()\n",
    "model_gru.add(Embedding(vocab_size, output_dim=32))\n",
    "model_gru.add(SpatialDropout1D(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "128b5a0d-e078-4083-af55-ab6243378a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU layer\n",
    "model_gru.add(GRU(32, reset_after=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9364b8e5-2446-4c10-8360-b419b8f176b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 32)          256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_2 (Spatia  (None, None, 32)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 32)                6240      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 262,273\n",
      "Trainable params: 262,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# dropout and dense layers\n",
    "model_gru.add(Dropout(0.4))\n",
    "model_gru.add(Dense(1, activation='sigmoid'))\n",
    "model_gru.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',metrics=['accuracy']\n",
    ")\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d65355-71fc-43b2-a39a-88934f502df7",
   "metadata": {},
   "source": [
    "We can see from the model summary that the number of parameters in the LSTM layer is 8320. A quick check can confirm that this is exactly four times the number of parameters in the plain RNN layer we saw in Exercise 6.01, Building and Training an RNN Model for Sentiment Classification, which is in line with our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "798d895a-ea4a-4293-a5c8-206c9042ab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "157/157 [==============================] - 11s 63ms/step - loss: 0.5636 - accuracy: 0.6888 - val_loss: 0.3922 - val_accuracy: 0.8392\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.3365 - accuracy: 0.8586 - val_loss: 0.3387 - val_accuracy: 0.8564\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.2795 - accuracy: 0.8895 - val_loss: 0.3105 - val_accuracy: 0.8740\n",
      "Epoch 4/5\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2472 - accuracy: 0.9056 - val_loss: 0.3726 - val_accuracy: 0.8486\n",
      "Epoch 5/5\n",
      "157/157 [==============================] - 10s 65ms/step - loss: 0.2265 - accuracy: 0.9140 - val_loss: 0.3019 - val_accuracy: 0.8852\n"
     ]
    }
   ],
   "source": [
    "# fit on training data for 5 epochs and batch size of 128\n",
    "history_gru = model_gru.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b13eda9c-a7b4-4e27-bce4-7f7066c4bf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.872\n"
     ]
    }
   ],
   "source": [
    "# predictions on test data\n",
    "y_test_pred_gru = (model_gru.predict(x_test) > 0.5).astype(\"int32\")\n",
    "print(accuracy_score(y_test, y_test_pred_gru))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af23b05-6532-4d73-94a9-c1253d689a90",
   "metadata": {},
   "source": [
    "We can see that our accuracy (87.1%) is close to (87.3%) from LSTMs. GRUs are simplifications of LSTMs that aim to provide similar accuracy with fewer parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5c5aa-9ad0-48fc-a6d5-d71a9bdd482b",
   "metadata": {},
   "source": [
    "#### Exercise 6.04: Bidirectional LSTM-Based Sentiment Classification Model\n",
    "In this exercise, we will use bidirectional LSTMs to predict sentiment on our data. We'll be using the bidirectional wrapper from Keras to create bidirectional layers on LSTMs (you could create a bidirectional GRU model by simply replacing LSTM with GRU in the wrapper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03b53b42-a22d-4ef2-a8f5-e16a19283456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional layer\n",
    "from tensorflow.keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29a31a43-e400-4e5f-8f34-f5a5525da394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential model + Embedding layer\n",
    "model_bilstm = Sequential()\n",
    "model_bilstm.add(Embedding(vocab_size, output_dim=32))\n",
    "model_bilstm.add(SpatialDropout1D(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f7dc887-bf48-484f-8f69-c40f0b1c9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional wrapper to an LSTM layer with 32 cells\n",
    "model_bilstm.add(Bidirectional(LSTM(32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b89f8263-015c-4956-9cb7-307400c27c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 32)          256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_3 (Spatia  (None, None, 32)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               16640     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 272,705\n",
      "Trainable params: 272,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dropout + Dense layers; compile and model summary\n",
    "model_bilstm.add(Dropout(0.4))\n",
    "model_bilstm.add(Dense(1,activation='sigmoid'))\n",
    "model_bilstm.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model_bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b6b9291-3616-4765-a56b-c246f3e6dbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "157/157 [==============================] - 15s 83ms/step - loss: 0.5609 - accuracy: 0.7082 - val_loss: 0.4243 - val_accuracy: 0.8272\n",
      "Epoch 2/4\n",
      "157/157 [==============================] - 14s 90ms/step - loss: 0.3400 - accuracy: 0.8650 - val_loss: 0.3282 - val_accuracy: 0.8678\n",
      "Epoch 3/4\n",
      "157/157 [==============================] - 15s 94ms/step - loss: 0.2716 - accuracy: 0.8962 - val_loss: 0.3040 - val_accuracy: 0.8706\n",
      "Epoch 4/4\n",
      "157/157 [==============================] - 15s 94ms/step - loss: 0.2432 - accuracy: 0.9082 - val_loss: 0.3184 - val_accuracy: 0.8828\n"
     ]
    }
   ],
   "source": [
    "# fit model_bilstm on training data for epochs=4, batch_size=128\n",
    "history_bilstm = model_bilstm.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    epochs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7eae1fa-1f27-456b-ac01-a479679fb632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87412\n"
     ]
    }
   ],
   "source": [
    "# predictions on test data\n",
    "y_test_pred_bilstm = (model_bilstm.predict(x_test) > 0.5).astype('int32')\n",
    "print(accuracy_score(y_test, y_test_pred_bilstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71480e0-2754-4699-9b35-2818608c6022",
   "metadata": {},
   "source": [
    "#### Exercise 6.05: Stacked LSTM-Based Sentiment Classification Model\n",
    "In this exercise, we will \"go deeper\" into the RNN architecture by stacking two LSTM layers to predict sentiment in our data. We will continue with the same setup that we used in the previous exercises (the number of cells, embedding dimensions, dropout, and so on) for the other layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66834c17-74bd-49d1-abd0-02c4079b3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential model; Embedding layer; SpatialDropout1D\n",
    "model_stack = Sequential()\n",
    "model_stack.add(Embedding(vocab_size, output_dim=32))\n",
    "model_stack.add(SpatialDropout1D(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac1c74eb-423a-43d5-bcff-8f51bdcaf111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer with 32 cells with return_sequences=True\n",
    "model_stack.add(LSTM(32, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a78cc4cc-2cab-4a25-bdf4-1b214a7464ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer with 32 cells\n",
    "model_stack.add(LSTM(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5898cd5-0357-4e2e-ac1c-0884cb67501f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 32)          256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_4 (Spatia  (None, None, 32)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 32)          8320      \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 272,673\n",
      "Trainable params: 272,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dropout layer; Dense layer; compile; model summary\n",
    "model_stack.add(Dropout(0.5))\n",
    "model_stack.add(Dense(1,activation='sigmoid'))\n",
    "model_stack.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model_stack.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5883b0e8-2f09-4926-8457-e9d9108edc2e",
   "metadata": {},
   "source": [
    "Note that the stacked LSTM model has the same number of parameters as the bidirectional model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "329a7a43-b224-4111-b2d0-d292a1ebd030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "157/157 [==============================] - 22s 123ms/step - loss: 0.5183 - accuracy: 0.7314 - val_loss: 0.3603 - val_accuracy: 0.8498\n",
      "Epoch 2/4\n",
      "157/157 [==============================] - 19s 121ms/step - loss: 0.3346 - accuracy: 0.8651 - val_loss: 0.3283 - val_accuracy: 0.8732\n",
      "Epoch 3/4\n",
      "157/157 [==============================] - 19s 123ms/step - loss: 0.2704 - accuracy: 0.8960 - val_loss: 0.2857 - val_accuracy: 0.8888\n",
      "Epoch 4/4\n",
      "157/157 [==============================] - 19s 122ms/step - loss: 0.2420 - accuracy: 0.9105 - val_loss: 0.2946 - val_accuracy: 0.8862\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on training data\n",
    "history_stack = model_stack.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    epochs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff3bd2f9-2ee9-4bdc-ac20-c30a15f91ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87652\n"
     ]
    }
   ],
   "source": [
    "# predictions on test set\n",
    "y_test_pred_stack = (model_stack.predict(x_test) > 0.5).astype('int32')\n",
    "print(accuracy_score(y_test, y_test_pred_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e032b04-95bd-4930-802a-38ec89b9870f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Activity 6.01: Sentiment Analysis of Amazon Product Reviews\n",
    "In this activity, we will build a sentiment classification model on Amazon product reviews. The data contains reviews for several categories of products. The original dataset, available at https://snap.stanford.edu/data/web-Amazon.html, is huge; therefore, we have sampled 50,000 reviews for this activity.\n",
    "\n",
    "The sampled dataset, which has been split into train and test sets, can be found at https://packt.live/3iNTUjN.\n",
    "\n",
    "This activity will bring together the concepts and methods we discussed in this chapter and those discussed in Chapter 4, Deep Learning for Text â€“ Embeddings, and Chapter 5, Deep Learning for Sequences. You will begin by performing a detailed text cleanup and conduct preprocessing to get it ready for the deep learning model. You will also use embeddings to represent text. For the prediction part, you will employ stacked LSTMs (two layers) and two dense layers."
   ]
  },
  {
   "cell_type": "raw",
   "id": "85781d39-2a19-48d9-ae5b-5ae6d9085226",
   "metadata": {},
   "source": [
    "detailed text cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29c2e0f2-04d7-4f1a-b40e-5ca764dcaf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/LNonyane/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/LNonyane/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# punkt and stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "165cb8e0-7244-4bb6-ba4f-6b760ff177a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41594e87-d172-4af9-8d48-59239f775a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing tokenizer on toy data\n",
    "sents = ['life is good', 'good life', 'good']\n",
    "# import + instantiate tokenizer and fit on toy data\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea579f30-3317-4e07-8a24-5c7698b82025",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.fit_on_texts(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "322db369-0e6a-4531-a84a-2dd0e049a146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 1], [1, 2], [1]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert input text to corresponding sequence of indices for terms\n",
    "tok.texts_to_sequences(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aab45b-0caf-40d3-86dc-b5337adb53f5",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67c33d63-54b4-4085-abdd-81c689d65bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 2), (25000, 2))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data files for the train and test sets\n",
    "amazon_train_data = pd.read_csv('Amazon_reviews_train.csv')\n",
    "amazon_test_data = pd.read_csv('Amazon_reviews_test.csv')\n",
    "\n",
    "# examine the shapes of data sets\n",
    "amazon_train_data.shape, amazon_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c36aad88-c7ae-4408-81b6-0975be5504eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review_text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "377cf78e-d8a4-4ef4-8801-58ea9eb75a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000,), (25000,))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate the data sets from labels\n",
    "train_raw = amazon_train_data['review_text'].values\n",
    "train_labels  = amazon_train_data['label'].values\n",
    "\n",
    "test_raw = amazon_test_data['review_text'].values\n",
    "test_labels  = amazon_test_data['label'].values\n",
    "\n",
    "# examine the shapes of the datasets\n",
    "train_raw.shape, train_labels.shape, test_raw.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9f22a-9d60-4ad0-b78b-88489f37c9f7",
   "metadata": {},
   "source": [
    "Normalize the case and tokenize the test and train texts using NLTK's word_tokenize (after importing it, of course â€“ hint: use list comprehension for cleaner code). Print the first review from the train data to check if the tokenization worked. Download punkt from NLTK if you haven't used the tokenizer before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "118c66de-26db-439b-89a6-feb59a5ba83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "64c23e98-3dbd-4088-aa28-b139721c5c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize case and tokenize\n",
    "# train data\n",
    "train_raw = [sent.lower() for sent in train_raw] # normalizing case\n",
    "train_raw_sents = [tokenize.sent_tokenize(sent) for sent in train_raw] # sent tokenizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fd8b0dba-c2e2-49cc-af7c-1cab2543a13b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " \"'stuning\",\n",
       " 'even',\n",
       " 'for',\n",
       " 'the',\n",
       " 'non-gamer',\n",
       " ':',\n",
       " 'this',\n",
       " 'sound',\n",
       " 'track',\n",
       " 'was',\n",
       " 'beautiful',\n",
       " '!',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'it\",\n",
       " 'paints',\n",
       " 'the',\n",
       " 'senery',\n",
       " 'in',\n",
       " 'your',\n",
       " 'mind',\n",
       " 'so',\n",
       " 'well',\n",
       " 'i',\n",
       " 'would',\n",
       " 'recomend',\n",
       " 'it',\n",
       " 'even',\n",
       " 'to',\n",
       " 'people',\n",
       " 'who',\n",
       " 'hate',\n",
       " 'vid',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'game\",\n",
       " 'music',\n",
       " '!',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'i',\n",
       " 'have',\n",
       " 'played',\n",
       " 'the',\n",
       " 'game',\n",
       " 'chrono',\n",
       " 'cross',\n",
       " 'but',\n",
       " 'out',\n",
       " 'of',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'games',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'played',\n",
       " 'it',\n",
       " 'has',\n",
       " 'the',\n",
       " 'best',\n",
       " 'music',\n",
       " '!',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'it\",\n",
       " 'backs',\n",
       " 'away',\n",
       " 'from',\n",
       " 'crude',\n",
       " 'keyboarding',\n",
       " 'and',\n",
       " 'takes',\n",
       " 'a',\n",
       " 'fresher',\n",
       " 'step',\n",
       " 'with',\n",
       " 'grate',\n",
       " 'guitars',\n",
       " 'and',\n",
       " 'soulful',\n",
       " 'orchestras',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'it\",\n",
       " 'would',\n",
       " 'impress',\n",
       " 'anyone',\n",
       " 'who',\n",
       " 'cares',\n",
       " 'to',\n",
       " 'listen',\n",
       " '!',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'^_^\",\n",
       " \"'\",\n",
       " ']']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data word tokenize\n",
    "train_raw_words = [tokenize.word_tokenize(str(sent)) for sent in train_raw_sents] # word tokenizing\n",
    "train_raw_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a3e32a01-e62b-4a5f-8df0-d6aea8982eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I\\'m in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life\\'s hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "62dcbb53-f625-431c-8b96-f009b63d4f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great cd: my lovely pat has one of the great voices of her generation.',\n",
       " 'i have listened to this cd for years and i still love it.',\n",
       " \"when i'm in a good mood it makes me feel better.\",\n",
       " 'a bad mood just evaporates like sugar in the rain.',\n",
       " 'this cd just oozes life.',\n",
       " 'vocals are jusat stuunning and lyrics just kill.',\n",
       " \"one of life's hidden gems.\",\n",
       " 'this is a desert isle cd in my book.',\n",
       " 'why she never made it big is just beyond me.',\n",
       " 'everytime i play this, no matter black, white, young, old, male, female everybody says one thing \"who was that singing ?\"']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize case and tokenize\n",
    "# test data\n",
    "test_raw = [sent.lower() for sent in test_raw] # normalizing case\n",
    "test_raw_sents = [tokenize.sent_tokenize(sent) for sent in test_raw] # sent tokenizing\n",
    "test_raw_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4de0d9c5-a48e-4fe6-816f-ea7b9a11f1fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " \"'great\",\n",
       " 'cd',\n",
       " ':',\n",
       " 'my',\n",
       " 'lovely',\n",
       " 'pat',\n",
       " 'has',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'great',\n",
       " 'voices',\n",
       " 'of',\n",
       " 'her',\n",
       " 'generation',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'i',\n",
       " 'have',\n",
       " 'listened',\n",
       " 'to',\n",
       " 'this',\n",
       " 'cd',\n",
       " 'for',\n",
       " 'years',\n",
       " 'and',\n",
       " 'i',\n",
       " 'still',\n",
       " 'love',\n",
       " 'it',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '``',\n",
       " 'when',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'in',\n",
       " 'a',\n",
       " 'good',\n",
       " 'mood',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'feel',\n",
       " 'better',\n",
       " '.',\n",
       " '``',\n",
       " ',',\n",
       " \"'\",\n",
       " 'a',\n",
       " 'bad',\n",
       " 'mood',\n",
       " 'just',\n",
       " 'evaporates',\n",
       " 'like',\n",
       " 'sugar',\n",
       " 'in',\n",
       " 'the',\n",
       " 'rain',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'this\",\n",
       " 'cd',\n",
       " 'just',\n",
       " 'oozes',\n",
       " 'life',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'vocals\",\n",
       " 'are',\n",
       " 'jusat',\n",
       " 'stuunning',\n",
       " 'and',\n",
       " 'lyrics',\n",
       " 'just',\n",
       " 'kill',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '``',\n",
       " 'one',\n",
       " 'of',\n",
       " 'life',\n",
       " \"'s\",\n",
       " 'hidden',\n",
       " 'gems',\n",
       " '.',\n",
       " '``',\n",
       " ',',\n",
       " \"'this\",\n",
       " 'is',\n",
       " 'a',\n",
       " 'desert',\n",
       " 'isle',\n",
       " 'cd',\n",
       " 'in',\n",
       " 'my',\n",
       " 'book',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'why\",\n",
       " 'she',\n",
       " 'never',\n",
       " 'made',\n",
       " 'it',\n",
       " 'big',\n",
       " 'is',\n",
       " 'just',\n",
       " 'beyond',\n",
       " 'me',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'everytime\",\n",
       " 'i',\n",
       " 'play',\n",
       " 'this',\n",
       " ',',\n",
       " 'no',\n",
       " 'matter',\n",
       " 'black',\n",
       " ',',\n",
       " 'white',\n",
       " ',',\n",
       " 'young',\n",
       " ',',\n",
       " 'old',\n",
       " ',',\n",
       " 'male',\n",
       " ',',\n",
       " 'female',\n",
       " 'everybody',\n",
       " 'says',\n",
       " 'one',\n",
       " 'thing',\n",
       " '``',\n",
       " 'who',\n",
       " 'was',\n",
       " 'that',\n",
       " 'singing',\n",
       " '?',\n",
       " \"''\",\n",
       " \"'\",\n",
       " ']']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data word tokenize\n",
    "test_raw_words = [tokenize.word_tokenize(str(sent)) for sent in test_raw_sents] # word tokenizing\n",
    "test_raw_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a98f91-c3a4-4705-84c4-79e5a7009ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function (drop_stop) to remove these tokens from any input (stopwords, punctuation) tokenized sentence.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "902a9869-a828-412a-b4b5-b7a9e1981930",
   "metadata": {},
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e87cc55c-2167-4aa3-85bf-f2a50db959b8",
   "metadata": {},
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
