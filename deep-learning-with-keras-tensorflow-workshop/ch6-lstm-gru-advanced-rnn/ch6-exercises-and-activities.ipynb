{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed824be-3789-4783-91b9-8cb09bcdd320",
   "metadata": {},
   "source": [
    "### In this chapter, we will use plain RNNs and variants of RNNs on a sentiment classification task: processing the input sequence and predicting whether the sentiment is positive or negative.\n",
    "\n",
    "We'll use the IMDb reviews dataset for this task. The dataset contains 50,000 movie reviews, along with their sentiment – 25,000 highly polar movie reviews for training and 25,000 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb299aa-8cb7-4af4-bce7-cb47b75a8a11",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd38d28d-d9ca-48b7-a63c-c9c2558fbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd1b59-ebd7-4391-b504-dca24094657f",
   "metadata": {},
   "source": [
    "With the module imported, importing the dataset (tokenized and separated into train and test sets) is as easy as running imdb.load_data. The only parameter we need to provide is the vocabulary size we wish to use.\n",
    "\n",
    "Here, we will specify a vocabulary size of 8,000 for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbd4bbb-cb12-411d-a300-6ce5272b00c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 4s 0us/step\n",
      "17473536/17464789 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8945d3-f9f3-431c-8bba-c98e18eed69e",
   "metadata": {},
   "source": [
    "Let's inspect the X_train variable to see what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171d1f18-9f11-48da-9fff-5a93463d3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "[1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 2, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 2, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(x_train[5]))\n",
    "print(x_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded045e-970a-41a6-8ba5-1c5144816732",
   "metadata": {},
   "source": [
    "The next step is to define an upper limit on the length of the sequences that we'll work with and limit all sequences to the defined maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b28971f-1895-4221-8c78-ba42a62409cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b4ecf-cd10-49f7-8137-66ad6ad44af8",
   "metadata": {},
   "source": [
    "The next step is to get all our sequences to the same length using the pad_sequences utility from Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d165980a-7d64-4dc5-90c5-e4c34cd16a81",
   "metadata": {},
   "source": [
    "#### Staging and Preprocessing Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a58fa-a443-44a8-8e13-f390af90ccb5",
   "metadata": {},
   "source": [
    "The pad_sequences utility from the sequences module in Keras helps us in getting all the sequences to a specified length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ae9027-a9b0-4f7b-8a35-1d9bec9b0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2756b352-bf4c-4e52-8420-6d74a8205ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374a10b4-9870-4001-946a-2106aa30f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    1  778  128   74   12  630  163   15    4 1766 7982\n",
      " 1051    2   32   85  156   45   40  148  139  121  664  665   10   10\n",
      " 1361  173    4  749    2   16 3804    8    4  226   65   12   43  127\n",
      "   24    2   10   10]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879314a9-b214-4639-8857-297e781545a2",
   "metadata": {},
   "source": [
    "We can see that there are plenty of 0s at the beginning of the result. As you may have inferred, this is the padding that's done by the pad_sequence utility because the input sequence was shorter than 200. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298b795-a94b-4db5-b254-2cbc161a4efc",
   "metadata": {},
   "source": [
    "#### The Embedding Layer\n",
    "The embedding layer is always the first layer in the model. You can follow it up with any architecture of your choice (RNNs, in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb0ebc-c5c0-43ec-b37b-cab92ac99f90",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Building the Plain RNN Model\n",
    "#### Exercise 6.01: Building and Training an RNN Model for Sentiment Classification\n",
    "In this exercise, we will build and train an RNN model for sentiment classification. Initially, we will define the architecture for the recurrent and prediction layers, and we will assess the model's performance on the test data. We will add the embedding layer and some dropout and complete the model definition by adding the RNN layer, dropout, and a dense layer to finish. Then, we'll check the accuracy of the predictions on the test data to assess how well the model generalizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e80bd6-8b47-4a88-ad9b-e8520bca760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for numpy and tensforflow for reproducible results\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439b6dc-a7a4-4f9b-b28b-15cc02b81d45",
   "metadata": {},
   "source": [
    "import all the necessary packages and layers and initializing a sequential model named model_rnn using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d259c20-4d64-41bd-a84f-12ff2210f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 06:10:19.888409: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Flatten, Dense, Embedding, SpatialDropout1D, Dropout\n",
    "model_rnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21cb16e8-8998-4864-be38-40a519aff79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer\n",
    "model_rnn.add(Embedding(vocab_size, output_dim=32))\n",
    "model_rnn.add(SpatialDropout1D(0.4)) # miniize overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13bb786b-89ee-486f-a750-1d1e5752d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleRNN layer\n",
    "model_rnn.add(SimpleRNN(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7e1d34a-1fed-4ba3-8a81-adb0681505c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout layer\n",
    "model_rnn.add(Dropout(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e4018b-6fdc-4786-b734-c2a3f2e59871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction layer (Dense)\n",
    "model_rnn.add(Dense(1, activation='sigmoid')) # sigmoid because we have binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83b9d53b-6920-4c26-bb2f-174233209275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          256000    \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, None, 32)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 258,113\n",
      "Trainable params: 258,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile the model and view summary\n",
    "model_rnn.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd38f8-53a7-4acf-9dbc-b55b8dcdf312",
   "metadata": {},
   "source": [
    "We can see that there are 258,113 parameters, most of which are present in the embedding layer. The reason for this is that the word embeddings are being learned during the training – so we're learning the embedding matrix, which is of dimensionality vocab_size(8000) × output_dim(32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6741778f-a37c-4a0e-ba94-c6233c75d311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6123 - accuracy: 0.6627 - val_loss: 0.4799 - val_accuracy: 0.7924\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.4404 - accuracy: 0.8126 - val_loss: 0.3820 - val_accuracy: 0.8378\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.3573 - accuracy: 0.8559 - val_loss: 0.3762 - val_accuracy: 0.8476\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.3201 - accuracy: 0.8755 - val_loss: 0.4821 - val_accuracy: 0.8222\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.2886 - accuracy: 0.8906 - val_loss: 0.3584 - val_accuracy: 0.8604\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.2703 - accuracy: 0.8976 - val_loss: 0.3396 - val_accuracy: 0.8714\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.2479 - accuracy: 0.9064 - val_loss: 0.6179 - val_accuracy: 0.8018\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.2319 - accuracy: 0.9123 - val_loss: 0.3673 - val_accuracy: 0.8632\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.2118 - accuracy: 0.9218 - val_loss: 0.3658 - val_accuracy: 0.8520\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.1940 - accuracy: 0.9281 - val_loss: 0.4071 - val_accuracy: 0.8262\n"
     ]
    }
   ],
   "source": [
    "# fit model on train data\n",
    "history_rnn = model_rnn.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2, # gives us a sense of the model performance on unseen data.\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91252f81-d356-4729-8466-d79e71109dcb",
   "metadata": {},
   "source": [
    "From the training output, we can see that the validation accuracy goes up to about 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "966a2156-6331-48b9-96af-183d1d653c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83172\n"
     ]
    }
   ],
   "source": [
    "# prediction on test data\n",
    "y_test_pred = (model_rnn.predict(x_test) > 0.5).astype('int32') # use this when working on binary classification.\n",
    "classes_x = np.argmax(y_test_pred, axis=1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d032a5-ec23-4354-917e-9e33204266e5",
   "metadata": {},
   "source": [
    "Overcome deprecated Sequential.predict_classes() method.\n",
    "\n",
    "Multi-class classification - Softmax last layer\n",
    "- np.argmax(model.predict(x), axis=-1)\n",
    "\n",
    "Binary classification - Sigmoid last layer\n",
    "- (model.predict(x) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00413695-5bb5-4fa3-aa98-6f79f029ab05",
   "metadata": {},
   "source": [
    "We can see that the model does a decent job. We used a simple architecture with 32 neurons and used a vocabulary size of just 8000. Tweaking these and other hyperparameters may get you better results and you are encouraged to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f999f45-713c-4575-8d70-c3f6986b8e71",
   "metadata": {},
   "source": [
    "In this exercise, we have seen how to build an RNN-based model for text. We saw how an embedding layer can be used to derive word vectors for the task at hand. These word vectors are the representations for each incoming term, which are passed to the RNN layer. We have seen that even a simple architecture can give us good results. Now, let's discuss how this model can be used to make predictions on new, unseen reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee96e2-916d-4f1d-baa9-eb930f5a0437",
   "metadata": {},
   "source": [
    "#### Making Predictions on Unseen Data\n",
    "Our model (model_rnn) was trained on IMDb reviews that were tokenized, had their case lowered, had punctuation removed, had a defined vocabulary size, and were converted into a sequence of indices. Our function/pipeline for preparing data for the RNN model needs to perform the same steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa37c87e-45bf-4bd7-855b-0c94cb437296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable containing raw review text\n",
    "inp_review = 'An excellent movie!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f7d237-615c-4cae-b778-5290e64960af",
   "metadata": {},
   "source": [
    "The sentiment in the text is positive. If the model is working well enough, it should predict the sentiment as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd0b720e-6dd1-4d7c-840a-bb566750facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d3f3f-cf15-4b07-a8db-b967b9e9cb29",
   "metadata": {},
   "source": [
    "The code above must tokenize this text into its constituent terms, normalize its case, and remove punctuation.\n",
    "\n",
    "Check if it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c7fc618-beab-4550-9d1e-4e689ff25fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'excellent', 'movie']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(inp_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8689b5ef-71cf-4178-9300-de5b03096c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n",
      "1654784/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load vacabulary into dictionary named word_map\n",
    "word_map = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a8a20b8-612f-49c8-a0bf-0787160ddee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the mapping to 8000 terms by sorting word_map variable on index and picking the first 8000 terms to match what is used on the train data.\n",
    "vocab_map = dict(sorted(word_map.items(), key=lambda x: x[1])[:vocab_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae412d9-156e-46ab-b34f-9bc5679f4004",
   "metadata": {},
   "source": [
    "The vocab map will be a dictionary containing the term for index mapping for the 8000 terms in the vocabulary. Using this mapping, we'll convert the tokenized sentence into a sequence of term indices by performing a lookup for each term and returning the corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11a88c56-7cea-434b-8683-798a84632983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess function that accepts raw text, applies the text_to_word_sequence utility to it, performs a lookup from vocab_map, and returns the corresponding sequence of integers\n",
    "def preprocess(review):\n",
    "    inp_tokens = text_to_word_sequence(review)\n",
    "    seq = []\n",
    "    for token in inp_tokens:\n",
    "        seq.append(vocab_map.get(token))\n",
    "        \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ace01179-b0ec-45ac-b6a1-e6acda576730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 318, 17]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(inp_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d6152-5577-40a1-b8e3-df62df66662d",
   "metadata": {},
   "source": [
    "This is the sequence of term indices corresponding to the raw text. Note that the data is now in the same format as the IMDb data we loaded. This sequence of indices can be fed to the RNN model (using the predict_classes method) to classify the sentiment, as shown in the following code. If the model is working well enough, it should predict the sentiment as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cb17aab-d23d-4498-a123-6965786eef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_rnn.predict([preprocess(inp_review)]) > 0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530650a-3303-409e-9217-6d1b8f295ae0",
   "metadata": {},
   "source": [
    "The output prediction is 1 (positive), just as we expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607dea26-bd79-454c-8c8c-cc86fda8b42a",
   "metadata": {},
   "source": [
    "Let's apply the function to another raw text review and supply it to the model for prediction. Let's update the inp_review variable so that it contains the text \"Don't watch this movie – poor acting, poor script, bad direction.\" The sentiment in the review is negative. We expect the model to classify it as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95a03414-0224-4eb9-8b98-5cd179259075",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_review = \"Don't watch this movie - poor acting, poor script, bad direction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69410297-526c-41db-8b71-aae495f92176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_rnn.predict([preprocess(inp_review)]) > 0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd910ea7-09d2-4fc7-9183-aa3700a3964e",
   "metadata": {},
   "source": [
    "The predicted sentiment is negative, just as we would expect the model to behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0caae-115b-4335-b7d6-3fa35b57dc48",
   "metadata": {},
   "source": [
    "#### LSTMs, GRUs, and Other Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe50305-32e7-4c90-b27a-6c395cf9c385",
   "metadata": {},
   "source": [
    "#### Exercise 6.02: LSTM-Based Sentiment Classification Model\n",
    "In this exercise, we will build a simple LSTM-based model to predict sentiment on our data. We will continue with the same setup we used previously (that is, the number of cells, embedding dimensions, dropout, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bc38761-5bd7-42c4-a0b4-59506696d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e55307-2856-4658-987f-4afb42073272",
   "metadata": {},
   "source": [
    "Instantiate the sequential model, add the embedding layer with the appropriate dimensions, and add a 40% spatial dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3dd9cd8-88f3-4f44-87e6-f803bbf9b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, output_dim=32))\n",
    "model_lstm.add(SpatialDropout1D(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f61bf3b5-c9d8-4f4d-951f-a98605ee8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "model_lstm.add(LSTM(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6916e17c-38dc-410b-88a9-b49e95b9d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 32)          256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, None, 32)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264,353\n",
      "Trainable params: 264,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# dropout and dense layers\n",
    "model_lstm.add(Dropout(0.4))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',metrics=['accuracy']\n",
    ")\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a61c91-77fa-4b81-bed6-63dbe7f3017b",
   "metadata": {},
   "source": [
    "We can see from the model summary that the number of parameters in the LSTM layer is 8320. A quick check can confirm that this is exactly four times the number of parameters in the plain RNN layer we saw in Exercise 6.01, Building and Training an RNN Model for Sentiment Classification, which is in line with our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfc79899-9312-4c54-a228-970cb4b6b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "157/157 [==============================] - 11s 64ms/step - loss: 0.5299 - accuracy: 0.7315 - val_loss: 0.4013 - val_accuracy: 0.8332\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.3298 - accuracy: 0.8709 - val_loss: 0.3489 - val_accuracy: 0.8568\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.2689 - accuracy: 0.8972 - val_loss: 0.2847 - val_accuracy: 0.8798\n",
      "Epoch 4/5\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.2397 - accuracy: 0.9112 - val_loss: 0.3224 - val_accuracy: 0.8832\n",
      "Epoch 5/5\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.2163 - accuracy: 0.9187 - val_loss: 0.2984 - val_accuracy: 0.8850\n"
     ]
    }
   ],
   "source": [
    "# fit on training data for 5 epochs and batch size of 128\n",
    "history_lstm = model_lstm.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29b1747a-e02c-4424-8363-b6e1a5e51360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87288\n"
     ]
    }
   ],
   "source": [
    "# test data performance\n",
    "y_test_pred_lstm = (model_lstm.predict(x_test) > 0.5).astype(\"int32\")\n",
    "print(accuracy_score(y_test, y_test_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ddc6fd-2ca1-4190-89c2-ad991b6b6112",
   "metadata": {},
   "source": [
    "The accuracy we got (87%) is a significant improvement from the accuracy we got using plain RNNs (83.2%). It looks like the extra parameters and the extra predictive power from the cell state came in handy for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8124698-340d-43ca-8240-42c2016866b2",
   "metadata": {},
   "source": [
    "#### Exercise 6.03: GRU-Based Sentiment Classification Model\n",
    "In this exercise, we will build a simple GRU-based model to predict sentiments in our data. We will continue with the same setup that we used previously (that is, the number of cells, embedding dimensions, dropout, and so on). Using GRUs instead of LSTMs in the model is as simple as replacing \"LSTM\" with \"GRU\" when adding the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e1f702a-b51d-4fa9-948c-44d98ea7c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GRU layer\n",
    "from tensorflow.keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f54b5180-af81-4358-b47d-5b0a0ebb0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru= Sequential()\n",
    "model_gru.add(Embedding(vocab_size, output_dim=32))\n",
    "model_gru.add(SpatialDropout1D(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "128b5a0d-e078-4083-af55-ab6243378a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU layer\n",
    "model_gru.add(GRU(32, reset_after=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9364b8e5-2446-4c10-8360-b419b8f176b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 32)          256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_3 (Spatia  (None, None, 32)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                6240      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 262,273\n",
      "Trainable params: 262,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# dropout and dense layers\n",
    "model_gru.add(Dropout(0.4))\n",
    "model_gru.add(Dense(1, activation='sigmoid'))\n",
    "model_gru.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',metrics=['accuracy']\n",
    ")\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d65355-71fc-43b2-a39a-88934f502df7",
   "metadata": {},
   "source": [
    "We can see from the model summary that the number of parameters in the LSTM layer is 8320. A quick check can confirm that this is exactly four times the number of parameters in the plain RNN layer we saw in Exercise 6.01, Building and Training an RNN Model for Sentiment Classification, which is in line with our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "798d895a-ea4a-4293-a5c8-206c9042ab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "157/157 [==============================] - 11s 64ms/step - loss: 0.5955 - accuracy: 0.6660 - val_loss: 0.3898 - val_accuracy: 0.8266\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.3539 - accuracy: 0.8535 - val_loss: 0.3309 - val_accuracy: 0.8626\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2851 - accuracy: 0.8875 - val_loss: 0.3078 - val_accuracy: 0.8690\n",
      "Epoch 4/5\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.2527 - accuracy: 0.9035 - val_loss: 0.3769 - val_accuracy: 0.8468\n",
      "Epoch 5/5\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.2287 - accuracy: 0.9128 - val_loss: 0.3106 - val_accuracy: 0.8792\n"
     ]
    }
   ],
   "source": [
    "# fit on training data for 5 epochs and batch size of 128\n",
    "history_gru = model_gru.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b13eda9c-a7b4-4e27-bce4-7f7066c4bf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87068\n"
     ]
    }
   ],
   "source": [
    "# predictions on test data\n",
    "y_test_pred_gru = (model_gru.predict(x_test) > 0.5).astype(\"int32\")\n",
    "print(accuracy_score(y_test, y_test_pred_gru))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af23b05-6532-4d73-94a9-c1253d689a90",
   "metadata": {},
   "source": [
    "We can see that our accuracy (87.1%) is close to (87.3%) from LSTMs. GRUs are simplifications of LSTMs that aim to provide similar accuracy with fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c0b856-c935-4201-b2bc-52b866e87c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
