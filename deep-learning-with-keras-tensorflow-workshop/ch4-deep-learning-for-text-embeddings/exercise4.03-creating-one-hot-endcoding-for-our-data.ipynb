{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d2a504-5c85-4f43-8f05-9dc1536fa56a",
   "metadata": {},
   "source": [
    "In this exercise, we will replicate the preceding example. The target terms are nlp, deep, and learn. We will create a one-hot encoded feature for these terms using our own function and store the result in a numpy array.\n",
    "\n",
    "Again, we'll be using the txt_words_nostop variable we created in Exercise 4.01, Tokenizing, Case Normalization, Punctuation, and Stop Word Removal. So, you will need to continue this exercise in the same Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae7c3b-a394-41b7-8121-a2118f34ca60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### exercise 4.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c533c2-0e83-4f1d-9f9d-666a8aa12c7d",
   "metadata": {},
   "source": [
    "In this exercise, we will remove stop words from the data, and also apply everything we have learned so far. We'll start by performing tokenization (sentences and words); then, we'll perform case normalization, followed by punctuation and stop word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ceeb76-1671-470e-b364-46e4c567d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/LNonyane/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/LNonyane/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# punkt and stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3667bd-13ca-4a22-b0ae-a94cfa2e47ad",
   "metadata": {},
   "source": [
    "We'll keep the code concise this time. We'll be defining and manipulating the raw_txt variable. Let's get started:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da5deb-170a-4018-a7ba-11df5a0df4de",
   "metadata": {},
   "source": [
    "Define the raw_txt variable so that it contains the text \"Welcome to the world of deep learning for NLP! We're in this together, and we'll learn together. NLP is amazing, and deep learning makes it even more fun. Let's learn!\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb4e344-191a-48cd-91d5-00fafb72c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_txt = \"\"\"\n",
    "Welcome to the world of deep learning for NLP! We're in this together, and we'll learn together. NLP is amazing, and deep learning makes it even more fun. Let's learn!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ecdc0-0aeb-4c89-b1e1-1ca8194eac7c",
   "metadata": {},
   "source": [
    "Use the sent_tokenize() method to separate the raw text into individual sentences and store the result in a variable. Use the lower() method to convert the string into lowercase before tokenizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e67c846-8e88-486f-b0cf-6f4e10b6d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_sents = tokenize.sent_tokenize(raw_txt.lower()) # calling sentence tokenizer on normalized raw_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06123147-5eb7-4f73-8844-c6144de2eafa",
   "metadata": {},
   "source": [
    "Using list comprehension, apply the word_tokenize() method to separate each sentence into its constituent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f9d5a0-a81a-4dcb-ad26-e2fec4f29c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents] # separate words to constituent parts for each sentence in txt_sents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb9e9b-8ff1-4cdf-814f-9577290309d8",
   "metadata": {},
   "source": [
    "Import punctuation from the string module and convert it into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645bc221-f0b3-41f7-ab5a-816266eaacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_punct = list(punctuation) # ! . ? [and many more]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f951b6-3dcf-4446-89db-b24bc87f99c1",
   "metadata": {},
   "source": [
    "Import the built-in stop words for English from NLTK and save them in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157223cf-b443-43ee-8c60-98b6ff2ef3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_nltk = stopwords.words('english') # they, my, mine, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c31b40d-a352-4dc8-99cd-513a472c5f62",
   "metadata": {},
   "source": [
    "Create a combined list that contains the punctuations as well as the NLTK stop words. Note that we can remove them together in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eaa1284-4183-4205-af7e-65d9f237d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_final = stop_punct + stop_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4556412-933b-4e07-ac27-9e615a3b8a6e",
   "metadata": {},
   "source": [
    "Define a function that will remove stop words and punctuation from the input sentence, provided as a collection of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15c80db2-8e60-495e-8477-b899fe426a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stop(input_tokens):\n",
    "    return [token for token in input_tokens if token not in stop_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b8eb9-ae3c-47bd-bc89-1459e5779177",
   "metadata": {},
   "source": [
    "Remove redundant tokens by applying the function to the tokenized sentences and store the result in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "925a1fa9-a511-488e-aa82-cbfb81652879",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_words_nostop = [drop_stop(sent) for sent in txt_words] # run drop_stop function on each sentence in txt_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c209ca-2fc1-4c41-947b-af31f39d946f",
   "metadata": {},
   "source": [
    "Print the first cleaned-up sentence from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cae6590-5d71-49fe-9ff0-8c69f897b238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'world', 'deep', 'learning', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "print(txt_words_nostop[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea17abd-0b91-49fb-a9c6-fef5532fa93b",
   "metadata": {},
   "source": [
    "In this exercise, we performed all the cleanup steps we've learned about so far. This time around, we combined certain steps and made the code more concise. These are some very common steps that we should apply when dealing with text data. You could try to further optimize and modularize by defining a function that returns the result after all the processing steps. We encourage you to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39fea30f-6a8f-4eca-9a33-f8b083f26eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_with_results_per_step(input_tokens):\n",
    "    print([token for token in input_tokens if token not in stop_final])\n",
    "    return [token for token in input_tokens if token not in stop_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb77d124-2439-4d55-a1aa-a5467181c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'world', 'deep', 'learning', 'nlp']\n",
      "[\"'re\", 'together', \"'ll\", 'learn', 'together']\n",
      "['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun']\n",
      "['let', \"'s\", 'learn']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['welcome', 'world', 'deep', 'learning', 'nlp'],\n",
       " [\"'re\", 'together', \"'ll\", 'learn', 'together'],\n",
       " ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'],\n",
       " ['let', \"'s\", 'learn']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[drop_with_results_per_step(sent) for sent in txt_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a789e8-66cc-47cf-a824-337a1a18b6e7",
   "metadata": {},
   "source": [
    "So far, the steps in the cleanup process were steps that got rid of tokens that weren't very useful in our assessment. But there are a few more things we could do to make our data even better â€“ we can try using our understanding of the language to combine tokens, identify tokens that have practically the same meaning, and remove further redundancy. A couple of popular approaches are stemming and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e6203-4a56-4122-be76-c749d0ab9f07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### exercise 4.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393da5f-a825-4752-93aa-0a4d5dfbfe7c",
   "metadata": {},
   "source": [
    "In this exercise, we will continue with data preprocessing. We removed the stop words and punctuation in the previous exercise. Now, we will use the Porter stemming algorithm to stem the tokens. Since we'll be using the txt_words_nostop variable we created previously, let's continue with the same Jupyter Notebook we created in Exercise 4.01, Tokenizing, Case Normalization, Punctuation, and Stop Word Removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9826bfe-32ce-43bf-8f5c-84ad719821f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import PorterStemmer from NLTK\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69008f5d-1477-416c-a17e-99cf9d285ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the stemmer\n",
    "stemmer_p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eda2e041-8298-4ad9-a993-de543f130cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcom', 'world', 'deep', 'learn', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "# Apply the stemmer to the first sentence in txt_words_nostop\n",
    "print([stemmer_p.stem(token) for token in txt_words_nostop[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2df9e73f-6140-49cb-b1c3-b86fe0c0f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the stemmer to all the sentences in the data using nested list comprehension\n",
    "txt_words_stem = [[stemmer_p.stem(token) for token in sent] for sent in txt_words_nostop]\n",
    "# run porter stem for each word in the sentences in txt_words_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "635a688d-8e80-4f5e-833c-d4ae67210f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['welcom', 'world', 'deep', 'learn', 'nlp'],\n",
       " [\"'re\", 'togeth', \"'ll\", 'learn', 'togeth'],\n",
       " ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],\n",
       " ['let', \"'s\", 'learn']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the output\n",
    "txt_words_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341b8e0-4fd2-4f2a-820b-f1bf58071d48",
   "metadata": {},
   "source": [
    "It looks like plenty of modifications have been made by the stemmer. Many of the words aren't valid anymore but are still recognizable, and that's okay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1faeb89-ba5d-42bd-be00-bfa973d46081",
   "metadata": {},
   "source": [
    "In this exercise, we used the Porter stemming algorithm to stem the terms of our tokenized data. Stemming works on individual terms, so it needs to be applied after tokenizing into terms. Stemming reduced some terms to their base form, which weren't necessarily valid English words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e580495e-b5bf-46af-9162-4224e895ee9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### exercise 4.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1bb418d-a5ca-4b66-b2f2-dd852751ce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['welcome', 'world', 'deep', 'learning', 'nlp'], [\"'re\", 'together', \"'ll\", 'learn', 'together'], ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'], ['let', \"'s\", 'learn']]\n"
     ]
    }
   ],
   "source": [
    "# Print out the txt_words_nostop variable to see what we're working with\n",
    "print(txt_words_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b8b3258-b944-41da-a9df-38f59617a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list with the target terms, that is, \"nlp\", \"deep\", \"learn\"\n",
    "target_terms = ['nlp', 'deep', 'learn']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76309b6f-e8ae-4d6a-b504-f576b041a4a5",
   "metadata": {},
   "source": [
    "Define a function that takes in a single tokenized sentence and returns a 0 or 1 for each target term, depending on its presence in the text. Note that the length of the output is fixed at 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da90e11e-9482-4a35-abe0-0dbb8059366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(sent):\n",
    "    return [1 if term in sent else 0 for term in target_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dd522d6-2be7-4754-a3b1-d8bcb619ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each sentence in our text and store the result in a variable\n",
    "one_hot_mat = [get_onehot(sent) for sent in txt_words_nostop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f348747-ff3e-44d4-94f7-aa9e6c7a40e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import numpy, create a numpy array from the result, and print it\n",
    "import numpy as np\n",
    "np.array(one_hot_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c25992-67a2-4d4f-8f35-d33021f26449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
