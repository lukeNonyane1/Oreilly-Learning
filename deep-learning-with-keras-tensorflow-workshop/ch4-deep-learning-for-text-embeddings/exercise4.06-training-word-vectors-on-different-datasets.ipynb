{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871d77e5-fa6d-4fa8-99e5-fed4de5c727a",
   "metadata": {},
   "source": [
    "In this exercise, we will train our own word vectors on the Brown corpus and the IMDb movie reviews corpus. We will assess the differences in the representations learned and the effect of the underlying training data. Follow these steps to complete this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3a9efd-7524-4ba4-91e1-9239bfc268a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df8a619-1537-469f-a4b2-35d0dd7d405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Brown and IMDb movie reviews corpus from NLTK:\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade3bdeb-482b-4d72-88b3-73b075d866c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/LNonyane/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/LNonyane/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import brown, movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4336ebd-9902-4b5a-acd8-3fb88d291fda",
   "metadata": {},
   "source": [
    "The corpora have a convenient method, sent(), to extract the individual sentences and words (tokenized sentences, which can be directly passed to the word2vec algorithm). Since both the corpora are rather small, use the Skip-gram method to create the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcf8693-6d64-4018-8619-2b2d29c1600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_brown = word2vec.Word2Vec(brown.sents(), sg=1)\n",
    "model_movie = word2vec.Word2Vec(movie_reviews.sents(), sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530acb7-091f-49ac-b71c-645cefd6cccc",
   "metadata": {},
   "source": [
    "We now have two embeddings that have been learned on different contexts for the same term. Let's see the most similar terms for money from the model on the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d6363e-66a1-49f9-9fd9-5fc768af6852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('care', 0.829934298992157),\n",
       " ('job', 0.8272829055786133),\n",
       " ('friendship', 0.8167790174484253),\n",
       " ('risk', 0.8044701218605042),\n",
       " ('joy', 0.8039414882659912)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print top 5 terms similar to money from model_brown\n",
    "model_brown.wv.most_similar('money', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8036e2-6bcb-4050-86e3-b624bf5c555a",
   "metadata": {},
   "source": [
    "We can see that the top term is 'care'; fair enough. Let's see what the model learned regarding movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e1745ad-0ebf-4d70-b398-e0d051b2a56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cash', 0.7237001061439514),\n",
       " ('paid', 0.7035631537437439),\n",
       " ('ransom', 0.6973373293876648),\n",
       " ('record', 0.6866511702537537),\n",
       " ('bucks', 0.6820871829986572)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print top 5 terms similar to money from model_movie\n",
    "model_movie.wv.most_similar('money', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6494502-aa42-4e79-8f86-d5d9af60eb96",
   "metadata": {},
   "source": [
    "The top terms are cash and paid. Considering the language being used in movies, and thus in movie reviews, this isn't very surprising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd80d4-606d-4864-96ff-7749723d0b27",
   "metadata": {},
   "source": [
    "In this exercise, we created word vectors using different datasets and saw that the representations for the same terms and the associations that were learned are very affected by the underlying data. So, choose your data wisely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b29bf65-d04f-4e46-a238-892cda9ecb5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using pre-trained word vectors\n",
    "So far, we've trained our own word embeddings using the small datasets we had access to. The folks at the Stanford NLP group have trained word embeddings on 6 billion tokens with 400,000 terms in the vocabulary. Individually, we will not have the resources to handle this scale. Fortunately, the Stanford NLP group has been benevolent enough to make these trained embeddings available to the general public so that people like us can benefit from their work. The trained embeddings are available on the GloVe page (https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f0e74c-dcb4-4956-b2ce-27526fa7eeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fm/b49y7nm968107m07lbt3cqsc0000gp/T/ipykernel_19781/1816828077.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.w2vformat.txt'\n",
    "glove2word2vec(\n",
    "    glove_input_file,\n",
    "    word2vec_output_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1014c0b5-46b3-46cb-b986-44b9d078aa8e",
   "metadata": {},
   "source": [
    "We specified the input and the output file and ran the glove2word2vec utility. As the name suggests, the utility takes in word vectors in GloVe format and converts them into word2vec format. After this, the word2vec models can understand these embeddings easily. Now, let's load the keyed word vectors from the text file (reformatted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4defffe-e9a2-49e2-85c5-5f4ded134ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\n",
    "    'glove.6B.100d.w2vformat.txt',\n",
    "    binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c6769-0320-4588-b6ad-96b67f6222ca",
   "metadata": {},
   "source": [
    "With this done, we have the GloVe embeddings in the model, along with all the handy utilities we had for the embeddings model from word2vec. Let's check out the top terms similar to \"money\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c795c3-6423-4f2e-a5d0-f91f0b917a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('funds', 0.8508071303367615),\n",
       " ('cash', 0.848483681678772),\n",
       " ('fund', 0.7594833374023438),\n",
       " ('paying', 0.7415367364883423),\n",
       " ('pay', 0.740767240524292)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(\n",
    "    'money',\n",
    "    topn=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8298c-3acf-4251-8f31-78785296bdd9",
   "metadata": {},
   "source": [
    "For closure, let's also check how this model performs on the king and queen tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8448d5a5-ea49-48bf-b6b1-c9a1f866e710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483),\n",
       " ('monarch', 0.6843380928039551),\n",
       " ('throne', 0.6755736470222473),\n",
       " ('daughter', 0.6594556570053101),\n",
       " ('princess', 0.6520534157752991)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(\n",
    "    positive=['woman', 'king'],\n",
    "    negative=['man'],\n",
    "    topn=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a4efa-a1e8-443a-aa47-7f1b26e62aea",
   "metadata": {},
   "source": [
    "Now that we have these embeddings in a model, we can work with them the same way we worked with the embeddings we created previously and can benefit from the larger dataset and vocabulary and the processing power used by the contributing organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0d80f-a9ea-4c48-96dc-902f5ea2f218",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Bias in embeddings\n",
    "It's great that the embeddings are capturing these regularities by learning from the text data. Let's try something similar to a profession. Let's see the term closest to doctor â€“ man + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0245c6a-79f6-4b8e-bf41-cdc15e0d4540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nurse', 0.7735227942466736),\n",
       " ('physician', 0.7189430594444275),\n",
       " ('doctors', 0.6824328303337097),\n",
       " ('patient', 0.6750683188438416),\n",
       " ('dentist', 0.6726033091545105)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(\n",
    "    positive=['woman', 'doctor'],\n",
    "    negative=['man'],\n",
    "    topn=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb23981-b2bd-4c41-bd0d-2046c7b1535f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
