{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e11bba4-4ba1-4e39-b0af-e9369d84bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/LNonyane/opt/anaconda3/lib/python3.9/site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/LNonyane/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/LNonyane/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/LNonyane/opt/anaconda3/lib/python3.9/site-packages (from gensim) (6.0.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e241edd-a2b4-470f-be30-83aec620fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b46b5d15-e087-4a00-ae85-049b071a6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53674f45-e7c2-442e-be2a-184cff44e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "dataset = word2vec.Text8Corpus('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d655dff8-2000-4a63-a3f5-2c4346761ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4331070a-3104-41ad-b2cf-3886fdd407f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729a766-4a76-45b2-87e3-cde06ab7403c",
   "metadata": {},
   "source": [
    "This may take a minute or two, or less, depending on your system. Once complete, we will have our trained word vectors in the model and have access to multiple handy utilities to work with these word vectors. Let's access the word vector/embedding for a term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6084b8c6-fb05-4fae-935f-6bed864571b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.20193794 -0.51874137 -0.91420144 -1.6580372   1.3939288   1.7950013\n",
      " -2.5248644  -2.4950335  -1.2926555  -0.63928956 -1.2239358  -1.8010147\n",
      "  0.24652585 -0.3728686  -1.1039593  -0.56072396  1.983839    0.37115866\n",
      "  0.93721384 -0.02463511  0.7784846   0.68078744 -1.4710312  -1.0266024\n",
      "  0.66701883 -3.7085626   2.061994    0.62728363 -1.1804457  -1.5666814\n",
      "  2.7032795   0.43801767  0.42981827  1.041845    1.8631765  -2.031059\n",
      "  0.38920856 -2.2387776   1.1209686  -0.9926629  -1.6059679  -0.4531603\n",
      " -0.38786557  0.20408194 -0.6060187  -1.3537091   0.8878299  -0.7379972\n",
      " -0.7410342   0.99285614  0.99263996  2.0742967  -0.58120114  1.7768904\n",
      "  1.1369077  -1.2211988   0.38107762  1.0072191  -2.1647868   2.1298537\n",
      "  0.8601733  -0.40261608 -1.7906673   2.3166728  -1.4726409  -0.86715883\n",
      "  1.0949367  -2.3150716  -0.6426731  -0.07227715 -1.5966916   1.8781711\n",
      "  0.3385172   2.13873    -0.2654786   0.14419995  0.1303719   1.3711656\n",
      " -1.2407774  -0.44129905  0.21813473 -1.2543632   0.43601993  0.83148783\n",
      " -0.6723907  -0.5823157  -0.2265175  -0.578724    1.3198472  -0.5704754\n",
      "  0.6497871  -0.17687216  0.36844292  1.5742266  -0.28839475  1.0120485\n",
      " -1.1042396  -0.7732277  -0.6628644   0.64691293]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['animal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a7d86-0206-400e-8d13-1634c6e00423",
   "metadata": {},
   "source": [
    "You have a series of numbers – the vector for the term\n",
    "Let's find the length of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e13aca-5e8f-428e-9b14-c1c34992d857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv['animal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee36a86-0202-4db9-ad3a-795a98bba34a",
   "metadata": {},
   "source": [
    "The representation for each term is now a vector of length 100 (the length is a hyperparameter we can change; we used the default setting to get started). The vector for any term can be accessed as we did previously. Among the other handy utilities is the most_similar() method, which helps us find the terms that are the most similar to a target term. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb3eea3-e1b5-4e77-8871-f7c85e892859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('animals', 0.7503687143325806),\n",
       " ('insect', 0.7306487560272217),\n",
       " ('humans', 0.6731089353561401),\n",
       " ('ants', 0.6644068360328674),\n",
       " ('organism', 0.6493010520935059),\n",
       " ('aquatic', 0.6490638256072998),\n",
       " ('feces', 0.6486082077026367),\n",
       " ('insects', 0.6470466256141663),\n",
       " ('mammal', 0.645098865032196),\n",
       " ('bees', 0.637350857257843)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('animal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1bcda-59c8-4404-babf-14fea66ffc48",
   "metadata": {},
   "source": [
    "The output is a list of tuples, with each tuple containing the term and its similarity score with the term \"animal\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cea738-d48c-4774-856e-05148785959b",
   "metadata": {},
   "source": [
    "Let's see what the model has learned as top terms related to \"happiness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edca67f2-4ecf-4159-b47a-2e497e5cd84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('humanity', 0.8003098368644714),\n",
       " ('pleasure', 0.7515838742256165),\n",
       " ('perfection', 0.7506499886512756),\n",
       " ('mankind', 0.7404095530509949),\n",
       " ('compassion', 0.7346247434616089),\n",
       " ('desires', 0.7338270545005798),\n",
       " ('goodness', 0.731827437877655),\n",
       " ('salvation', 0.7279468178749084),\n",
       " ('dignity', 0.7221781611442566),\n",
       " ('striving', 0.7101019024848938)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('happiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b5464-abc4-49da-9323-8f407cafdeab",
   "metadata": {},
   "source": [
    "#### Semantic Regularities in Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23f64a-038a-4b71-abcc-7907c54a1263",
   "metadata": {},
   "source": [
    "We'll use the most_similar() method here, which allows us to add and subtract vectors from each other. We'll provide 'king' and 'woman' as vectors to add to each other, use 'man' to subtract from the result, and then check out the five terms that are the most similar to the resulting vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a2a673b-57db-42e7-b0c6-2bd49baf7835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6848368644714355),\n",
       " ('prince', 0.6343647241592407),\n",
       " ('princess', 0.6249984502792358),\n",
       " ('throne', 0.6246045827865601),\n",
       " ('emperor', 0.6148173809051514)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\n",
    "    positive=['woman', 'king'],\n",
    "    negative=['man'],\n",
    "    topn=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfc8c2-90b3-4c1f-a7c5-362233739ac4",
   "metadata": {},
   "source": [
    "The top result is 'queen'. Looks like the model is capturing these regularities. Let's try out another example. \"Man\" is to \"uncle\" as \"woman\" is to ? Or in an arithmetic form, what is the vector closest to uncle - man + woman = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "308539e6-d47c-48ae-9365-21987daa3f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aunt', 0.819966197013855),\n",
       " ('grandmother', 0.8063263893127441),\n",
       " ('wife', 0.797320544719696),\n",
       " ('widow', 0.787786066532135),\n",
       " ('daughter', 0.777454674243927)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\n",
    "    positive=['uncle', 'woman'],\n",
    "    negative=['man'],\n",
    "    topn=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2711126c-db3f-4881-8aae-294604062be2",
   "metadata": {},
   "source": [
    "This seems to be working great. Notice that all the top five results are for the feminine gender. So, we took uncle, removed the masculine elements, added feminine elements, and now we have some really good results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcdceab-ee43-4e89-820b-3d8ac04df1d7",
   "metadata": {},
   "source": [
    "#### Exercise 4.05 - Vectors for Phrases\n",
    "In this exercise, we will begin to create vectors for two different phrases, get happy and make merry, by taking the average of the individual vectors. We will find a similarity between the representations for the phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b787c0d0-4112-414f-ac77-f06dfe90f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the vector for the term \"get\" and store it in a variable\n",
    "v1 = model.wv['get']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c10368-e06d-4105-adcf-47ea68483f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the vector for the term \"happy\" and store it in a variable\n",
    "v2 = model.wv['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c4d930b-1822-4257-b3b9-edf600f2c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector as the element-wise average of the two vectors, (v1 + v2)/2. This is our vector for the entire phrase \"get happy\"\n",
    "res1 = (v1 + v2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db7ccdf4-4b7c-4dd7-9950-e84472625e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make merry vectors\n",
    "v1, v2 = model.wv['make'], model.wv['merry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dba9c60a-69b7-4aad-9e8c-f7fe84ad6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make merry vector phrase\n",
    "res2 = (v1+v2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee936d52-121f-42c5-a84b-3a6dfc2ea119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5528542], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the cosine_similarities() method in the model, find the cosine similarity between the two\n",
    "model.wv.cosine_similarities(res1, [res2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db26e66b-a7b1-4783-8bae-8070f0d25226",
   "metadata": {},
   "source": [
    "The result is a cosine similarity of about 0.57, which is positive and much higher than 0. This means that the model thinks the phrases \"get happy\" and \"make merry\" are similar in meaning. \n",
    "In this exercise, we saw how we could use vector arithmetic to represent phrases, instead of individual terms, and we saw that meaning is still captured. This brings us to a very important lesson – vector arithmetic on word embeddings has meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40fca29-8fd6-4780-906c-82fe641fc8a8",
   "metadata": {},
   "source": [
    "#### Effect of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b0912b9-99d4-44b7-b49b-9110068b70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's retrain the word embeddings, with size as 30 this time\n",
    "model = word2vec.Word2Vec(\n",
    "    dataset,\n",
    "    vector_size=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20656363-2ee2-432a-abe8-0dc05645d47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('empress', 0.8121461272239685),\n",
       " ('son', 0.8082416653633118),\n",
       " ('judah', 0.8058237433433533),\n",
       " ('emperor', 0.8039002418518066),\n",
       " ('prince', 0.8004615306854248)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, let's check the analogy task from earlier, that is, king - man + woman\n",
    "model.wv.most_similar(\n",
    "    positive=['woman', 'king'],\n",
    "    negative=['man'],\n",
    "    topn=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f18b1-5456-4809-8c0c-4dbbea8b4eca",
   "metadata": {},
   "source": [
    "We can see that queen isn't present in the top five results. It looks like by using a very low dimensionality, we aren't capturing enough information in the representation for a term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa8b20-9a60-48f4-8890-6598a2e0d329",
   "metadata": {},
   "source": [
    "#### Skip-gram versus CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64aff5e8-af3a-44d9-9cc2-9fd4e1239104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nieve', 0.7358265519142151),\n",
       " ('whimsical', 0.7154113054275513),\n",
       " ('diaghilev', 0.7142093181610107),\n",
       " ('nstlerroman', 0.7102295756340027),\n",
       " ('demian', 0.7060530781745911)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find most similar terms for the word, 'oeuvre' (boday of work of an artist/performer), using default vector_size\n",
    "model = word2vec.Word2Vec(dataset)\n",
    "model.wv.most_similar(\n",
    "    'oeuvre',\n",
    "    topn=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec0242-99d0-4f6e-a55d-899e7bacee7f",
   "metadata": {},
   "source": [
    "We can see that most results are the names of artists (alcaeus, loesser, and respighi) or art forms (naturae, sonnet). None of the top five results are close in meaning to the target term. Now, let's retrain our vectors using the Skip-gram method and see the result on the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67c9baa1-cd9a-46c3-8c4f-58dec3727181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('masterful', 0.8247474431991577),\n",
       " ('cubist', 0.8234925866127014),\n",
       " ('inimitable', 0.8187395334243774),\n",
       " ('impressionistic', 0.8166742324829102),\n",
       " ('lithographs', 0.8158390522003174)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg = word2vec.Word2Vec(\n",
    "    dataset,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "model_sg.wv.most_similar(\n",
    "    'oeuvre',\n",
    "    topn=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d030d-a1e3-4a83-852f-3e58afa661c1",
   "metadata": {},
   "source": [
    "We can see that the top terms are much closer in meaning (masterful, cubist, impressionistic). So, the Skip-gram method seems to work better for rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3a3ca-e860-4462-938e-f820eb32d42f",
   "metadata": {},
   "source": [
    "#### Bias in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a13de40a-b4ad-4568-99b1-68c2522683ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nurse', 0.6096571683883667),\n",
       " ('child', 0.5963546633720398),\n",
       " ('teacher', 0.5726506114006042),\n",
       " ('prostitute', 0.5636792778968811),\n",
       " ('helen', 0.5335953831672668)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\n",
    "    positive=['woman', 'doctor'],\n",
    "    negative=['man'],\n",
    "    topn=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f324af9-ae18-4b35-8d4e-1f159afa1a7f",
   "metadata": {},
   "source": [
    "That's not the kind of result we want. Doctors are males, while females are nurses? Let's try another example. This time, let's try what the model thinks regarding females as corresponding to \"smart\" for \"males\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f29d8b9-c89e-42e5-9391-67660b2b3339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lazy', 0.5782583355903625),\n",
       " ('dumb', 0.5647369027137756),\n",
       " ('rodeo', 0.5478234887123108),\n",
       " ('wee', 0.5471178889274597),\n",
       " ('lucille', 0.5415554046630859)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\n",
    "    positive=['woman', 'smart'],\n",
    "    negative=['man'],\n",
    "    topn=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78ebb9-1429-4a86-9227-178c764ca137",
   "metadata": {},
   "source": [
    "What's happening here? Is this seemingly great representation approach sexist? Is the word2vec algorithm sexist? There definitely is bias in the resulting word vectors, but think about where the bias is coming from. It's the underlying data that uses 'nurse' for females in contexts where 'doctor' is used for males. It is, therefore, the underlying text that contains the bias, not the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0231c-f1d3-4416-96bf-d69894a14c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
