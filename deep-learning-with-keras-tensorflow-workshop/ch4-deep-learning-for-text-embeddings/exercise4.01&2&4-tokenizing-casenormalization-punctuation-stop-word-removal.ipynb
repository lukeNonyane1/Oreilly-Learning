{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ab1b87-951a-47a1-8188-511c50294032",
   "metadata": {},
   "source": [
    "In this exercise, we will remove stop words from the data, and also apply everything we have learned so far. We'll start by performing tokenization (sentences and words); then, we'll perform case normalization, followed by punctuation and stop word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e77fb9-df8a-44db-b6f0-e88ef661b793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/LNonyane/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/LNonyane/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# punkt and stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6ed2b-e28a-4e45-bef6-8b1d1b7fe661",
   "metadata": {},
   "source": [
    "We'll keep the code concise this time. We'll be defining and manipulating the raw_txt variable. Let's get started:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448e019-d03f-45e7-b941-a7d5835d39b8",
   "metadata": {},
   "source": [
    "Define the raw_txt variable so that it contains the text \"Welcome to the world of deep learning for NLP! We're in this together, and we'll learn together. NLP is amazing, and deep learning makes it even more fun. Let's learn!\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beef080d-22e3-419c-b5d8-3efa704d2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_txt = \"\"\"\n",
    "Welcome to the world of deep learning for NLP! We're in this together, and we'll learn together. NLP is amazing, and deep learning makes it even more fun. Let's learn!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a37307c-0c40-4e6c-84a6-d58c78d1765d",
   "metadata": {},
   "source": [
    "Use the sent_tokenize() method to separate the raw text into individual sentences and store the result in a variable. Use the lower() method to convert the string into lowercase before tokenizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6d5141-5c3e-4b29-ad32-8aa3f201466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_sents = tokenize.sent_tokenize(raw_txt.lower()) # calling sentence tokenizer on normalized raw_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566eaed-635f-4e57-843a-c4ac82e9cf4a",
   "metadata": {},
   "source": [
    "Using list comprehension, apply the word_tokenize() method to separate each sentence into its constituent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5890d5a-a541-4a3d-8880-72fe441b0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents] # separate words to constituent parts for each sentence in txt_sents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0552bef-4918-4a77-b7ec-a53c25d975ee",
   "metadata": {},
   "source": [
    "Import punctuation from the string module and convert it into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4686686-82ca-4d2d-9c72-3fcb7de2458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_punct = list(punctuation) # ! . ? [and many more]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e072f480-42cd-40e5-8698-eb43a9eacb33",
   "metadata": {},
   "source": [
    "Import the built-in stop words for English from NLTK and save them in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f451f7b-b7a2-49f3-bb32-2fe0cb71730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_nltk = stopwords.words('english') # they, my, mine, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e620de-dee5-42bd-8488-aac5670bee28",
   "metadata": {},
   "source": [
    "Create a combined list that contains the punctuations as well as the NLTK stop words. Note that we can remove them together in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7531c8f1-8e7a-4e1e-8bf6-20724caf7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_final = stop_punct + stop_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90028aea-20af-4284-a38f-7cb7487e577b",
   "metadata": {},
   "source": [
    "Define a function that will remove stop words and punctuation from the input sentence, provided as a collection of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1d5951-a75d-42bf-bbeb-2a53865e5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stop(input_tokens):\n",
    "    return [token for token in input_tokens if token not in stop_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e5299-e48a-429f-b130-cb566d999041",
   "metadata": {},
   "source": [
    "Remove redundant tokens by applying the function to the tokenized sentences and store the result in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3902063f-e9fd-4c00-b57d-3f7ef9346713",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_words_nostop = [drop_stop(sent) for sent in txt_words] # run drop_stop function on each sentence in txt_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f48b3-6c21-4472-a21b-7bca8219057a",
   "metadata": {},
   "source": [
    "Print the first cleaned-up sentence from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "480938aa-08cc-438f-baf8-1d3e6b26da68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'world', 'deep', 'learning', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "print(txt_words_nostop[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc587493-e2c4-47cc-bb39-45d4fd0396d6",
   "metadata": {},
   "source": [
    "In this exercise, we performed all the cleanup steps we've learned about so far. This time around, we combined certain steps and made the code more concise. These are some very common steps that we should apply when dealing with text data. You could try to further optimize and modularize by defining a function that returns the result after all the processing steps. We encourage you to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c2bda7-43cc-4515-9fba-21af5f592459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_with_results_per_step(input_tokens):\n",
    "    print([token for token in input_tokens if token not in stop_final])\n",
    "    return [token for token in input_tokens if token not in stop_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "443b3c0e-6cb4-4890-8fe0-987a27c8cdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'world', 'deep', 'learning', 'nlp']\n",
      "[\"'re\", 'together', \"'ll\", 'learn', 'together']\n",
      "['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun']\n",
      "['let', \"'s\", 'learn']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['welcome', 'world', 'deep', 'learning', 'nlp'],\n",
       " [\"'re\", 'together', \"'ll\", 'learn', 'together'],\n",
       " ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'],\n",
       " ['let', \"'s\", 'learn']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[drop_with_results_per_step(sent) for sent in txt_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521259f6-6d34-48a9-9403-ec8b17de2f6f",
   "metadata": {},
   "source": [
    "So far, the steps in the cleanup process were steps that got rid of tokens that weren't very useful in our assessment. But there are a few more things we could do to make our data even better – we can try using our understanding of the language to combine tokens, identify tokens that have practically the same meaning, and remove further redundancy. A couple of popular approaches are stemming and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3726d83-0048-413d-8b63-1c80cb52279c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### exercise 4.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a67ccda-5ec1-43ee-9267-81796e50d32d",
   "metadata": {},
   "source": [
    "In this exercise, we will continue with data preprocessing. We removed the stop words and punctuation in the previous exercise. Now, we will use the Porter stemming algorithm to stem the tokens. Since we'll be using the txt_words_nostop variable we created previously, let's continue with the same Jupyter Notebook we created in Exercise 4.01, Tokenizing, Case Normalization, Punctuation, and Stop Word Removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e3b78b-b29a-49e2-b3bc-82cfe6da50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import PorterStemmer from NLTK\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f60f7907-53f8-4a41-8000-9aede65c3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the stemmer\n",
    "stemmer_p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "777cc377-2036-4cd6-948f-56385f8e2f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcom', 'world', 'deep', 'learn', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "# Apply the stemmer to the first sentence in txt_words_nostop\n",
    "print([stemmer_p.stem(token) for token in txt_words_nostop[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fce2b6ba-baf8-44b0-8b61-bfccfb0ec019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the stemmer to all the sentences in the data using nested list comprehension\n",
    "txt_words_stem = [[stemmer_p.stem(token) for token in sent] for sent in txt_words_nostop]\n",
    "# run porter stem for each word in the sentences in txt_words_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f664873-ae43-4e9d-b00d-374eebceda08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['welcom', 'world', 'deep', 'learn', 'nlp'],\n",
       " [\"'re\", 'togeth', \"'ll\", 'learn', 'togeth'],\n",
       " ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],\n",
       " ['let', \"'s\", 'learn']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the output\n",
    "txt_words_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2dcda-de4b-4e47-bac6-984ba8589470",
   "metadata": {},
   "source": [
    "It looks like plenty of modifications have been made by the stemmer. Many of the words aren't valid anymore but are still recognizable, and that's okay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b5409c-370a-4b67-9803-c5c3641f3312",
   "metadata": {},
   "source": [
    "In this exercise, we used the Porter stemming algorithm to stem the terms of our tokenized data. Stemming works on individual terms, so it needs to be applied after tokenizing into terms. Stemming reduced some terms to their base form, which weren't necessarily valid English words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd3c73c-4e43-410c-a9e6-c36b02ef1169",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### End of exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06979ef-c593-4014-95c1-055f6812904b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1391ace8-8a15-4785-8dbc-184ce55af897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nwelcome to the world of deep learning for nlp!',\n",
       " \"we're in this together, and we'll learn together.\",\n",
       " 'nlp is amazing, and deep learning makes it even more fun.',\n",
       " \"let's learn!\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fe93630-e9c1-4062-94d9-e7a033cecbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CountVectorizer function \n",
    "from sklearn.feature_extraction.text import CountVectorizer # utility can work with raw text as well as tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b5de36e-c47e-4432-aad2-846629ad75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instansitate vectorizer and provide the vocabulary size. This picks the top n terms from the data for creating the matrix (Document Term Matrix)\n",
    "vectorizer = CountVectorizer(max_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1eca27f5-2ee0-4756-b47d-82636cf26cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deep': 1, 'we': 4, 'together': 3, 'and': 0, 'learn': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teach the vectorizer the vocabulary - top n terms - and print them out.\n",
    "vectorizer.fit(txt_sents)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77c72f92-994a-448c-a731-cdfe6219c78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 2, 2],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now, let's apply the vectorizer to the data to create the DTM. \n",
    "A minor detail: the result from a vectorizer is a sparse matrix. \n",
    "To view it, we'll convert it into an array\n",
    "\"\"\"\n",
    "txt_dtm = vectorizer.fit_transform(txt_sents)\n",
    "txt_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8bb238d-86d9-4d5c-a7e4-e3800c1d5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notice that the vectorizer tokenizes the sentence as well. \n",
    "If you don't want that and want to use preprocessed tokens instead (txt_words_stem), \n",
    "you simply need to pass a dummy tokenizer and preprocessor to CountVectorizer.\n",
    "First, we create a function that does nothing and simply returns the tokenized sentence/document\n",
    "\"\"\"\n",
    "def do_nothing(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fc86cf6-727b-43aa-8fcd-20ed5c6885bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we'll instantiate the vectorizer to use this function as the preprocessor and tokenizer\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=5,\n",
    "    preprocessor=do_nothing,\n",
    "    tokenizer=do_nothing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80ab7c16-187e-4834-b2bc-8455c675c60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 0, 2],\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here, we're fitting and transforming the data in one step using the fit_transform() method from the tokenizer, \n",
    "and then we view the result. The method identifies the unique terms as the vocabulary when fitting on the data, \n",
    "then counts and returns the occurrence of each term for each document when transforming. \n",
    "Let's see it in action\n",
    "\"\"\"\n",
    "\n",
    "txt_dtm = vectorizer.fit_transform(txt_words_stem)\n",
    "txt_dtm.toarray()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2baade69-9ea2-4c25-85fc-79c5264ee281",
   "metadata": {},
   "source": [
    "We can see that the output is different from that of the previous result. Is this difference expected? To understand, let's look at the vocabulary of the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bb7ba1a-b8aa-49a0-a597-053c523f4fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deep': 1, 'learn': 2, 'nlp': 3, 'togeth': 4, \"'ll\": 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3bdfef9-ee69-43a3-ada0-1917d2b74d6d",
   "metadata": {},
   "source": [
    "We're working with preprocessed data, remember? We have already removed stop words and stemmed. Let's try printing out the input data just to be sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a2af1cc-cbc5-4150-b177-f1a663510e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['welcom', 'world', 'deep', 'learn', 'nlp'],\n",
       " [\"'re\", 'togeth', \"'ll\", 'learn', 'togeth'],\n",
       " ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],\n",
       " ['let', \"'s\", 'learn']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_words_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9fab9-f443-49f4-8548-a7da0aed8bfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Exercise 4.04: Document-Term Matrix with TF-IDF"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b89f1ea3-e6aa-482d-a77b-1ae6b659ad0e",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, we'll implement the third approach to feature generation from text – TF-IDF. \n",
    "We will use scikit-learn's TfidfVectorizer utility and create the DTM for our raw text data. \n",
    "Since we're using the txt_sents variable we created earlier in this chapter, \n",
    "we'll need to use the same Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16a7a8aa-4c62-4dd3-8594-2ed6a414058a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nwelcome to the world of deep learning for nlp!',\n",
       " \"we're in this together, and we'll learn together.\",\n",
       " 'nlp is amazing, and deep learning makes it even more fun.',\n",
       " \"let's learn!\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddfda3c1-e8e6-4a5b-85d5-8de4cc727b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TfidfVectorizer utility from scikit learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "808b762b-4194-44c2-af49-7a88ddef92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the vectorizer with a vocabulary size of 5\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd25666b-c7c6-4c1e-86fe-d4205e516fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer on the raw data of txt_sents:\n",
    "vectorizer_tfidf.fit(txt_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de260e75-650a-4059-b1bd-b84e063d9241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deep': 1, 'we': 4, 'together': 3, 'and': 0, 'learn': 2}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the vocabulary learned by the vectorizer\n",
    "vectorizer_tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6acf9de-1e57-40a8-8969-66fa65904604",
   "metadata": {},
   "source": [
    "Notice that the vocabulary is the same as that of the count vectorizer. This is expected. We're not changing the vocabulary; we're adjusting its importance for the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e82460a4-82f0-4b5f-87cc-c4eae58de60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data using the trained vectorizer\n",
    "txt_tfidf = vectorizer_tfidf.transform(txt_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "522b2b54-bd69-4245-a1f9-d936f2d5024b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.25932364, 0.        , 0.25932364, 0.65783832, 0.65783832],\n",
       "       [0.70710678, 0.70710678, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the resulting DTM\n",
    "txt_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dd98852-3524-435c-86b3-f209dba87e50",
   "metadata": {},
   "source": [
    "We can clearly see that the output values are different from the frequencies and that the values less than 1 indicate that many values have been lowered after multiplication with IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1b3196a-bbc5-445b-abd5-17c144b6a0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.51082562, 1.51082562, 1.51082562, 1.91629073, 1.91629073])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We also need to see the IDF for each of the terms in the vocabulary to check if the factor is indeed working as we expect it to. \n",
    "Print out the IDF values for the terms using the idf_ attribute\n",
    "\"\"\"\n",
    "\n",
    "vectorizer_tfidf.idf_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b997d8be-10d3-434e-a41c-7b8bfea0eef7",
   "metadata": {},
   "source": [
    "The terms 'and', 'deep', and 'learn' have a lower IDF, while the terms 'together' and 'we' have a higher IDF. This is just as we expect it to be – the terms 'together' and 'we' appear only in one document, while the others appear in two. So, the TF-IDF scheme is indeed giving more importance to rarer words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe01759-2346-443f-96c7-9d4375113120",
   "metadata": {},
   "source": [
    "In this exercise, we saw how we can represent text using the TF-IDF approach. We also saw how the approach downweighs more frequent terms by noticing that the IDF values were lower for higher-frequency terms. We ended up with a DTM containing the TF-IDF values for the terms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
